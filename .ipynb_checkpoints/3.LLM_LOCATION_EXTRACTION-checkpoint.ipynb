{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Necessary libraries"
      ],
      "metadata": {
        "id": "Us1_Q99Jqy1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzNL0DWBuxxt",
        "outputId": "9ee01dd9-3988-46da-a389-5147138e99e9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m235.5/235.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S28XZQ0x5QKK",
        "outputId": "64b8a96b-d868-4733-f9fc-b063ca2f9444"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.108.0)\n",
            "Collecting openai\n",
            "  Downloading openai-1.108.1-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.9)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Downloading openai-1.108.1-py3-none-any.whl (948 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.4/948.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.108.0\n",
            "    Uninstalling openai-1.108.0:\n",
            "      Successfully uninstalled openai-1.108.0\n",
            "Successfully installed openai-1.108.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import unidecode\n",
        "from io import StringIO\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import time\n",
        "import ast"
      ],
      "metadata": {
        "id": "s7zURbdba_OO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Paragraphs to feed into LLM"
      ],
      "metadata": {
        "id": "RHalxtNSq6Td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_sentence_paragraphs(text, sentences_per_paragraph=6):\n",
        "    # Basic sentence splitter\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "    paragraphs = []\n",
        "    for i in range(0, len(sentences), sentences_per_paragraph):\n",
        "        para = \" \".join(sentences[i:i+sentences_per_paragraph])\n",
        "        paragraphs.append(para)\n",
        "    return paragraphs\n",
        "\n",
        "def txt_file_to_page_paragraphs(txt_path, sentences_per_paragraph=6):\n",
        "    \"\"\"\n",
        "    Reads a .txt file with \"--- Page N ---\" markers,\n",
        "    returns a list of dicts: {\"filename\", \"page\", \"paragraph\"}\n",
        "    \"\"\"\n",
        "    filename = os.path.basename(txt_path)\n",
        "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Split by page marker\n",
        "    page_blocks = re.split(r'\\n?--- Page (\\d+) ---\\n', text)\n",
        "    out = []\n",
        "    # page_blocks[0] is any text before the first page (often empty)\n",
        "    for i in range(1, len(page_blocks), 2):\n",
        "        page_num = int(page_blocks[i])\n",
        "        page_text = page_blocks[i+1]\n",
        "        paragraphs = text_to_sentence_paragraphs(page_text, sentences_per_paragraph)\n",
        "        for para in paragraphs:\n",
        "            if para.strip():\n",
        "                out.append({\n",
        "                    \"filename\": filename,\n",
        "                    \"page\": page_num,\n",
        "                    \"paragraph\": para.strip()\n",
        "                })\n",
        "    return out\n",
        "\n",
        "def folder_txts_to_page_paragraphs(folder_path, sentences_per_paragraph=6):\n",
        "    \"\"\"\n",
        "    For all .txt files in folder, returns a list of dicts:\n",
        "    filename, page, paragraph\n",
        "    \"\"\"\n",
        "    all_paragraphs = []\n",
        "    for fname in os.listdir(folder_path):\n",
        "        if fname.lower().endswith(\".txt\"):\n",
        "            file_path = os.path.join(folder_path, fname)\n",
        "            out = txt_file_to_page_paragraphs(file_path, sentences_per_paragraph)\n",
        "            all_paragraphs.extend(out)\n",
        "    return all_paragraphs"
      ],
      "metadata": {
        "id": "5tgglRQIgeLE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"/content/drive/MyDrive/Data_txt\"\n",
        "paragraphs = folder_txts_to_page_paragraphs(folder_path)"
      ],
      "metadata": {
        "id": "0pf5n7kI1W9Y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4Byf5JZ9xdD",
        "outputId": "f67510e3-c1ff-4fb0-fe12-8301b833578f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: openai\n",
            "Version: 1.108.1\n",
            "Summary: The official Python library for the openai API\n",
            "Home-page: https://github.com/openai/openai-python\n",
            "Author: \n",
            "Author-email: OpenAI <support@openai.com>\n",
            "License: Apache-2.0\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: anyio, distro, httpx, jiter, pydantic, sniffio, tqdm, typing-extensions\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# API key\n",
        "os.environ['OPENAI_API_KEY'] = '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'\n",
        "\n",
        "# Testing API connection\n",
        "client = OpenAI()\n",
        "try:\n",
        "    model_list = client.models.list()\n",
        "    print(\"API connection successful! Number of models:\", len(model_list.data))\n",
        "except Exception as e:\n",
        "    print(\"API connection failed:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XH1-yuImCCR",
        "outputId": "7a7c899a-a6f5-4e8d-c657-ee54c37b877e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API connection successful! Number of models: 83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few shot Prompts"
      ],
      "metadata": {
        "id": "O58uMGovrDzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  FEW SHOT PROMPT FOR GEO LOCALITY EXTRACTION\n",
        "few_shot_prompts = \"\"\"\n",
        "You are an assistant that extracts locality descriptions from geoscience texts.\n",
        "A locality description can be:\n",
        "- a site, region, province, district, city, towns, villages, or a GPS coordinate (e.g., UTM, lat/lon).\n",
        "- Do NOT generate or invent localities. Only extract those *explicitly mentioned* in the paragraph.\n",
        "- If there are no localities mentioned, output: []\n",
        "\n",
        "For each paragraph, extract the *relevant locality descriptions* as a list. If none, output: None.\n",
        "Example 1:\n",
        "Paragraph: The NE-trending Markoye fault in Burkina Faso is a first-order crustal scale structure located in the north-eastern part of Burkina Faso,between the towns of Dori and Tambão by the border with Mali and Niger.\n",
        "Localities: [\"north-eastern part of Burkina Faso\", \"towns of Dori and Tambão\", \"border with Mali and Niger\"]\n",
        "\n",
        "Example 2:\n",
        "Paragraph: Several east-west traverses were conducted across the Markoye fault between Essakane and Tambão. Field stations were established at the villages of Salmossi, Essakane, Tin Agadel, Tin Taradat, Markoye and Gorom-Gorom.\n",
        "Localities: [\"Markoye fault\", \"Essakane\", \"Tambão\", \"Salmossi\", \"Tin Agadel\", \"Tin Taradat\", \"Markoye\", \"Gorom-Gorom\"]\n",
        "\n",
        "Example 3:\n",
        "Paragraph: The area of study is located ±250 km NE of the capital city, Ouagadougou, Burkina Faso (Fig. 1). The extent of the study area is demarcated by the villages of Essakane to the south-west, Tin Agadel and Tin Taradat to the north-east, and Markoye and Gorom-Gorom to the east.\n",
        "Localities: [\"250 km NE of Ouagadougou, Burkina Faso\", \"Essakane\", \"Tin Agadel\", \"Tin Taradat\", \"Markoye\", \"Gorom-Gorom\"]\n",
        "\n",
        "Example 4:\n",
        "Paragraph: The UTM coordinates of the field area are: 30 P 0805000 E, 1600000 N; 0805000 E, 1594000 N; 0810000 E, 1594000 N; 0810000 E, 1600000 N.\n",
        "Localities: [\"30 P 0805000 E, 1600000 N\", \"0805000 E, 1594000 N\", \"0810000 E, 1594000 N\", \"0810000 E, 1600000 N\"]\n",
        "\n",
        "Example 5:\n",
        "Paragraph: No specific localities or coordinates are mentioned in this paragraph.\n",
        "Localities: []\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "NeLcisXAmEM8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_localities_llm(paragraph, model=\"gpt-4o\"):\n",
        "    prompt = (\n",
        "        few_shot_prompts +\n",
        "        f\"\\nParagraph: {paragraph}\\nLocalities:\"\n",
        "    )\n",
        "    response = openai.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=120,\n",
        "        temperature=0,\n",
        "    )\n",
        "    answer = response.choices[0].message.content.strip()\n",
        "    # Try to extract the list\n",
        "    m = re.search(r\"\\[(.*?)\\]\", answer, re.DOTALL)\n",
        "    if m:\n",
        "        raw_list = m.group(0)\n",
        "        try:\n",
        "            extracted = ast.literal_eval(raw_list)\n",
        "            if isinstance(extracted, str):  # Occasionally LLM returns string not list\n",
        "                extracted = [extracted]\n",
        "        except:\n",
        "            extracted = []\n",
        "    else:\n",
        "        extracted = []\n",
        "    return extracted\n",
        "\n",
        "def extract_from_paragraphs_w_page(paragraphs, model=\"gpt-4o\"):\n",
        "    if not paragraphs:\n",
        "        print(\"No paragraphs provided.\")\n",
        "        return []\n",
        "    print(f\"\\nProcessing: {paragraphs[0]['filename']} ({len(paragraphs)} paragraphs)\")\n",
        "    results = []\n",
        "    for p in paragraphs:\n",
        "        para = p['paragraph']\n",
        "        page = p['page']\n",
        "        filename = p['filename']\n",
        "        if len(para.strip()) < 10:\n",
        "            continue\n",
        "        retry = 0\n",
        "        while True:\n",
        "            try:\n",
        "                localities = extract_localities_llm(para, model=model)\n",
        "                if localities:\n",
        "                    results.append({\n",
        "                        \"filename\": filename,\n",
        "                        \"page\": page,\n",
        "                        \"mention\": para,\n",
        "                        \"location\": localities\n",
        "                    })\n",
        "                break\n",
        "            except openai.RateLimitError:\n",
        "                print(\"Rate limit hit, waiting 3 seconds...\")\n",
        "                time.sleep(3)\n",
        "                retry += 1\n",
        "                if retry > 5:\n",
        "                    print(\"Too many retries, skipping this paragraph.\")\n",
        "                    break\n",
        "    return results"
      ],
      "metadata": {
        "id": "xv_OoNG4oQ9y"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_extract_localities_llm_paragraphs(paragraphs, output_folder, model=\"gpt-4o\"):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    # Group paragraphs by filename\n",
        "    from collections import defaultdict\n",
        "    by_file = defaultdict(list)\n",
        "    for p in paragraphs:\n",
        "        by_file[p['filename']].append(p)\n",
        "\n",
        "    for fname, plist in by_file.items():\n",
        "        out_csv = os.path.join(output_folder, fname.replace(\".txt\", \"_locations_llm.csv\"))\n",
        "        if os.path.exists(out_csv):\n",
        "            print(f\"Already processed {fname}, skipping.\")\n",
        "            continue\n",
        "        print(f\"Extracting for {fname} ({len(plist)} paragraphs)\")\n",
        "        results = []\n",
        "        for p in plist:\n",
        "            para = p['paragraph']\n",
        "            page = p['page']\n",
        "            retry = 0\n",
        "            while True:\n",
        "                try:\n",
        "                    locs = extract_localities_llm(para, model=model)\n",
        "                    if locs:\n",
        "                        for loc in locs:\n",
        "                            results.append({\n",
        "                                \"filename\": fname,\n",
        "                                \"page\": page,\n",
        "                                \"mention\": para,\n",
        "                                \"location\": loc\n",
        "                            })\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    print(f\"Error: {e}. Sleeping...\")\n",
        "                    time.sleep(4)\n",
        "                    retry += 1\n",
        "                    if retry > 5:\n",
        "                        print(\"Failed after many retries, skipping paragraph.\")\n",
        "                        break\n",
        "        df = pd.DataFrame(results)\n",
        "        df.to_csv(out_csv, index=False)\n",
        "        print(f\"Saved {len(df)} locations to {out_csv}\")\n",
        "\n",
        "\n",
        "paragraphs = folder_txts_to_page_paragraphs(\"/content/drive/MyDrive/Data_txt\", sentences_per_paragraph=6)\n",
        "batch_extract_localities_llm_paragraphs(paragraphs, \"/content/drive/MyDrive/Data_csv_llm\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZBNox31sbe2",
        "outputId": "9bf95d32-8627-4719-a056-a3c377637ab9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting for 2013_Peters.txt (262 paragraphs)\n",
            "Saved 364 locations to /content/drive/MyDrive/Data_csv_llm/2013_Peters_locations_llm.csv\n",
            "Extracting for 2015_Masurel_phd.txt (647 paragraphs)\n",
            "Saved 995 locations to /content/drive/MyDrive/Data_csv_llm/2015_Masurel_phd_locations_llm.csv\n",
            "Extracting for 2013_FUNYUFUNYU.txt (270 paragraphs)\n",
            "Saved 313 locations to /content/drive/MyDrive/Data_csv_llm/2013_FUNYUFUNYU_locations_llm.csv\n",
            "Extracting for 2014_MSc_YOSSI.txt (165 paragraphs)\n",
            "Saved 384 locations to /content/drive/MyDrive/Data_csv_llm/2014_MSc_YOSSI_locations_llm.csv\n",
            "Extracting for 2015_LeBrun_Siguiri.txt (497 paragraphs)\n",
            "Saved 990 locations to /content/drive/MyDrive/Data_csv_llm/2015_LeBrun_Siguiri_locations_llm.csv\n",
            "Extracting for 2008_MATABANE_FE3.txt (107 paragraphs)\n",
            "Saved 136 locations to /content/drive/MyDrive/Data_csv_llm/2008_MATABANE_FE3_locations_llm.csv\n",
            "Extracting for 2011_Peters_East Markoye_2011.txt (164 paragraphs)\n",
            "Saved 106 locations to /content/drive/MyDrive/Data_csv_llm/2011_Peters_East Markoye_2011_locations_llm.csv\n",
            "Extracting for 2010_Matsheka_Irvin Final Thesis.txt (85 paragraphs)\n",
            "Saved 118 locations to /content/drive/MyDrive/Data_csv_llm/2010_Matsheka_Irvin Final Thesis_locations_llm.csv\n",
            "Extracting for 2013_Ramabulana_Sadiola Hill petrology.txt (96 paragraphs)\n",
            "Saved 123 locations to /content/drive/MyDrive/Data_csv_llm/2013_Ramabulana_Sadiola Hill petrology_locations_llm.csv\n",
            "Extracting for 2007_Tshibubudze_THE MARKOYE FAULT_2007.txt (116 paragraphs)\n",
            "Saved 146 locations to /content/drive/MyDrive/Data_csv_llm/2007_Tshibubudze_THE MARKOYE FAULT_2007_locations_llm.csv\n",
            "Extracting for 2009_Bontle Nkuna_0605886P_Honours Report.txt (95 paragraphs)\n",
            "Saved 105 locations to /content/drive/MyDrive/Data_csv_llm/2009_Bontle Nkuna_0605886P_Honours Report_locations_llm.csv\n",
            "Extracting for 2010_Mohale_GIS interpretation of NE Burkina Faso.txt (87 paragraphs)\n",
            "Saved 89 locations to /content/drive/MyDrive/Data_csv_llm/2010_Mohale_GIS interpretation of NE Burkina Faso_locations_llm.csv\n",
            "Extracting for 2011_Woolfe_The stratigraphy and metamorphic facies of the KEMB.txt (152 paragraphs)\n",
            "Saved 56 locations to /content/drive/MyDrive/Data_csv_llm/2011_Woolfe_The stratigraphy and metamorphic facies of the KEMB_locations_llm.csv\n",
            "Extracting for 2012_Simoko_Petrology, geochemistry and structure of the Pissila batholith and the Saaba Zone gneiss.txt (90 paragraphs)\n",
            "Saved 47 locations to /content/drive/MyDrive/Data_csv_llm/2012_Simoko_Petrology, geochemistry and structure of the Pissila batholith and the Saaba Zone gneiss_locations_llm.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning and deduplicating the locations"
      ],
      "metadata": {
        "id": "KqKJExkarJas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_location(loc):\n",
        "    if pd.isna(loc):\n",
        "        return \"\"\n",
        "    return unidecode.unidecode(str(loc).strip())\n",
        "\n",
        "def agg_pages(series):\n",
        "    return ','.join(sorted({str(x) for x in series if pd.notna(x)}))\n",
        "\n",
        "def agg_mentions(series):\n",
        "    return \" || \".join(sorted({str(x).strip() for x in series if pd.notna(x)}))\n",
        "\n",
        "def deduplicate_llm_csv(input_csv, output_csv):\n",
        "    df = pd.read_csv(input_csv)\n",
        "    if 'location' not in df.columns:\n",
        "        print(f\"Skipped {input_csv} (no 'location' column)\")\n",
        "        return\n",
        "    df['location'] = df['location'].map(clean_location)\n",
        "    df_dedup = (\n",
        "        df.groupby(['filename', 'location'])\n",
        "        .agg({\n",
        "            'page': agg_pages,\n",
        "            'mention': agg_mentions\n",
        "        })\n",
        "        .reset_index()\n",
        "        .rename(columns={'page': 'page', 'mention': 'mention'})\n",
        "    )\n",
        "    df_dedup = df_dedup[['filename','page', 'mention','location']]\n",
        "    df_dedup['source']=\"llm\"\n",
        "    df_dedup.to_csv(output_csv, index=False)\n",
        "    print(f\"Saved deduped: {output_csv}\")\n",
        "\n",
        "def batch_clean_locations(input_folder, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    for fname in os.listdir(input_folder):\n",
        "        if fname.lower().endswith(\".csv\"):\n",
        "            inpath = os.path.join(input_folder, fname)\n",
        "            outname = os.path.splitext(fname)[0] + \"_clean.csv\"\n",
        "            outpath = os.path.join(output_folder, outname)\n",
        "            try:\n",
        "                deduplicate_llm_csv(inpath, outpath)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {fname}: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "input_folder = \"/content/drive/MyDrive/Data_csv_llm\"\n",
        "output_folder = \"/content/drive/MyDrive/Data_LLM_CSV_clean\"\n",
        "\n",
        "batch_clean_locations(input_folder, output_folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUkOeRjR0sCW",
        "outputId": "b657e3ca-b887-4b37-c6d4-ab56b94ad0ea"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved deduped: /content/drive/MyDrive/Data_LLM_CSV_clean/2013_Peters_locations_llm_clean.csv\n",
            "Saved deduped: /content/drive/MyDrive/Data_LLM_CSV_clean/2015_Masurel_phd_locations_llm_clean.csv\n",
            "Saved deduped: /content/drive/MyDrive/Data_LLM_CSV_clean/2013_FUNYUFUNYU_locations_llm_clean.csv\n",
            "Saved deduped: /content/drive/MyDrive/Data_LLM_CSV_clean/2014_MSc_YOSSI_locations_llm_clean.csv\n",
            "Saved deduped: /content/drive/MyDrive/Data_LLM_CSV_clean/2015_LeBrun_Siguiri_locations_llm_clean.csv\n",
            "Saved deduped: /content/drive/MyDrive/Data_LLM_CSV_clean/2008_MATABANE_FE3_locations_llm_clean.csv\n",
            "Saved deduped: /content/drive/MyDrive/Data_LLM_CSV_clean/2011_Peters_East Markoye_2011_locations_llm_clean.csv\n",
            "Saved deduped: /content/drive/MyDrive/Data_LLM_CSV_clean/2010_Matsheka_Irvin Final Thesis_locations_llm_clean.csv\n",
            "Saved deduped: /content/drive/MyDrive/Data_LLM_CSV_clean/2013_Ramabulana_Sadiola Hill petrology_locations_llm_clean.csv\n",
            "Saved deduped: /content/drive/MyDrive/Data_LLM_CSV_clean/2007_Tshibubudze_THE MARKOYE FAULT_2007_locations_llm_clean.csv\n",
            "Saved deduped: /content/drive/MyDrive/Data_LLM_CSV_clean/2009_Bontle Nkuna_0605886P_Honours Report_locations_llm_clean.csv\n",
            "Saved deduped: /content/drive/MyDrive/Data_LLM_CSV_clean/2010_Mohale_GIS interpretation of NE Burkina Faso_locations_llm_clean.csv\n",
            "Saved deduped: /content/drive/MyDrive/Data_LLM_CSV_clean/2011_Woolfe_The stratigraphy and metamorphic facies of the KEMB_locations_llm_clean.csv\n",
            "Saved deduped: /content/drive/MyDrive/Data_LLM_CSV_clean/2012_Simoko_Petrology, geochemistry and structure of the Pissila batholith and the Saaba Zone gneiss_locations_llm_clean.csv\n"
          ]
        }
      ]
    }
  ]
}