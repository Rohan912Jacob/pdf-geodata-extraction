{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWvHgZWo9Pxv",
        "outputId": "cbfd0dfb-41c0-413a-cc15-6018261e283a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting geotext\n",
            "  Downloading geotext-0.4.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Downloading geotext-0.4.0-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: geotext\n",
            "Successfully installed geotext-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install geotext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import io\n",
        "import csv\n",
        "import os\n",
        "import pandas as pd\n",
        "from typing import List, Tuple\n",
        "from collections import defaultdict\n",
        "import spacy\n",
        "from geotext import GeoText"
      ],
      "metadata": {
        "id": "0USNmvGO9ffx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_sentence_paragraphs(text, sentences_per_paragraph=6):\n",
        "    # Basic sentence splitter\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "    paragraphs = []\n",
        "    for i in range(0, len(sentences), sentences_per_paragraph):\n",
        "        para = \" \".join(sentences[i:i+sentences_per_paragraph])\n",
        "        paragraphs.append(para)\n",
        "    return paragraphs\n",
        "\n",
        "def txt_file_to_page_paragraphs(txt_path, sentences_per_paragraph=6):\n",
        "    \"\"\"\n",
        "    Reads a .txt file with \"--- Page N ---\" markers,\n",
        "    returns a list of dicts: {\"filename\", \"page\", \"paragraph\"}\n",
        "    \"\"\"\n",
        "    filename = os.path.basename(txt_path)\n",
        "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Split by page marker\n",
        "    page_blocks = re.split(r'\\n?--- Page (\\d+) ---\\n', text)\n",
        "    out = []\n",
        "    # page_blocks[0] is any text before the first page (often empty)\n",
        "    for i in range(1, len(page_blocks), 2):\n",
        "        page_num = int(page_blocks[i])\n",
        "        page_text = page_blocks[i+1]\n",
        "        paragraphs = text_to_sentence_paragraphs(page_text, sentences_per_paragraph)\n",
        "        for para in paragraphs:\n",
        "            if para.strip():\n",
        "                out.append({\n",
        "                    \"filename\": filename,\n",
        "                    \"page\": page_num,\n",
        "                    \"paragraph\": para.strip()\n",
        "                })\n",
        "    return out\n",
        "\n",
        "def folder_txts_to_page_paragraphs(folder_path, sentences_per_paragraph=6):\n",
        "    \"\"\"\n",
        "    For all .txt files in folder, returns a list of dicts:\n",
        "    filename, page, paragraph\n",
        "    \"\"\"\n",
        "    all_paragraphs = []\n",
        "    for fname in os.listdir(folder_path):\n",
        "        if fname.lower().endswith(\".txt\"):\n",
        "            file_path = os.path.join(folder_path, fname)\n",
        "            out = txt_file_to_page_paragraphs(file_path, sentences_per_paragraph)\n",
        "            all_paragraphs.extend(out)\n",
        "    return all_paragraphs"
      ],
      "metadata": {
        "id": "1b7yYtkrDkcz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "COMPASS = {\"N\",\"S\",\"E\",\"W\",\"NE\",\"NW\",\"SE\",\"SW\"}\n",
        "DENY_SINGLE = {\n",
        "    \"al.\", \"et\", \"al\", \"date\", \"university\", \"université\", \"ministere\", \"orpailleur\", \"brgm\",\n",
        "    \"thièblemont\", \"yacouba\"\n",
        "}\n",
        "DENY_TITLES = {\"university\", \"declaration\", \"preamble\", \"chapter\", \"figure\", \"table\"}\n",
        "DENY_PHRASE_PREFIXES = (\n",
        "    \"geologists at\", \"in this region\", \"various geological features\",\n",
        "    \"between the towns\", \"the region\", \"the desert\", \"the capital city\"\n",
        ")\n",
        "\n",
        "def looks_like_location_token(t: str) -> bool:\n",
        "    s = t.strip()\n",
        "    if not s: return False\n",
        "    if len(s) <= 2: return False\n",
        "    if s.upper() in COMPASS: return False\n",
        "    if s.lower() in DENY_SINGLE: return False\n",
        "    if s.endswith(\".\"): return False\n",
        "    if re.fullmatch(r\"[^\\w]+\", s): return False\n",
        "    return True\n",
        "\n",
        "def looks_like_location_phrase(t: str) -> bool:\n",
        "    s = re.sub(r\"\\s+\", \" \", t.strip())\n",
        "    if not looks_like_location_token(s):\n",
        "        return False\n",
        "    low = s.lower()\n",
        "    if low in DENY_TITLES: return False\n",
        "    if any(low.startswith(pref) for pref in DENY_PHRASE_PREFIXES): return False\n",
        "    if s.isupper() and \" \" not in s and not re.search(r\"[-’']\", s):\n",
        "        return False\n",
        "    words = s.split()\n",
        "    if len(words) >= 2:\n",
        "        titleish = sum(w[:1].isupper() for w in words) >= 1\n",
        "        if not titleish: return False\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "IvN_bdwQ_XAr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extractor 1: Using Spacy\n",
        "\n",
        "Loads spaCy’s small english model and extracts entities with labels GPE, LOC, FAC (cities/countries/regions + many sites like mines)."
      ],
      "metadata": {
        "id": "qrt9jfkKAPCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spacy_extract_locations(paragraphs):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    out = []\n",
        "    for item in paragraphs:\n",
        "        filename = item['filename']\n",
        "        page = item['page']\n",
        "        para = item['paragraph']\n",
        "        doc = nlp(para)\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in {\"GPE\", \"LOC\", \"FAC\"}:\n",
        "                loc_text = ent.text.strip()\n",
        "                if looks_like_location_token(loc_text):\n",
        "                    out.append({\n",
        "                        \"filename\": filename,\n",
        "                        \"page\": page,\n",
        "                        \"mention\": para,\n",
        "                        \"location\": loc_text,\n",
        "                        \"label\": ent.label_,\n",
        "                        \"source\": \"spacy\"\n",
        "                    })\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "lhAza6vJDdYA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extractor 2: Using Geotext"
      ],
      "metadata": {
        "id": "pJXhZcvdFTDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def geotext_extract_locations(paragraphs):\n",
        "    \"\"\"\n",
        "    For each paragraph dict (filename, page, paragraph),\n",
        "    extract city/country mentions using GeoText.\n",
        "    Output: list of dicts with filename, page, mention (the paragraph), location, and label (\"GEO\").\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    for item in paragraphs:\n",
        "        filename = item['filename']\n",
        "        page = item['page']\n",
        "        para = item['paragraph']\n",
        "        places = GeoText(para)\n",
        "        # Combine all found locations\n",
        "        locs = set(list(places.cities) + list(places.countries))\n",
        "        for loc in locs:\n",
        "            if looks_like_location_token(loc):   # Use your custom filter!\n",
        "                out.append({\n",
        "                    \"filename\": filename,\n",
        "                    \"page\": page,\n",
        "                    \"mention\": para,\n",
        "                    \"location\": loc,\n",
        "                    \"label\": \"GEO\",\n",
        "                    \"source\": \"geotext\"\n",
        "                })\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "93eaQxGxGyGz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extractor 3: REGEX"
      ],
      "metadata": {
        "id": "zpTGcJy4HeTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def regex_geo_spans(text: str) -> List[Tuple[int, int, str, str, str]]:\n",
        "    \"\"\"\n",
        "    Find geographic phrases using regex patterns.\n",
        "\n",
        "    Matches three main cases:\n",
        "      1. Proper name(s) followed by a geo suffix (e.g. \"Ashanti Belt\", \"West African Craton\")\n",
        "      2. Directional phrase + proper name (e.g. \"West Africa\", \"Eastern Highlands\")\n",
        "      3. Proper name(s) + geo unit (e.g. \"Essakane Mine\", \"Tarkwa Basin\", \"Pilbara Region\")\n",
        "\n",
        "    Returns a list of spans: (start_index, end_index, source, label, text)\n",
        "    \"\"\"\n",
        "\n",
        "    # case-insensitive suffixes that define a geo phrase\n",
        "    geo_suffixes = (\n",
        "        r\"(?i:\"  # (?i:) = case-insensitive group\n",
        "        r\"Belt|Greenstone Belt|Craton|Basin|Shear Zone|Fault|Goldfield|Range|Desert|River|\"\n",
        "        r\"Lake|Sea|Ocean|Gulf|Province|State|Region|Valley|Plateau|Peninsula|Archipelago|\"\n",
        "        r\"Canyon|Strait|Channel|Highlands|Lowlands|Orogeny|Greenstone\"\n",
        "        r\")\"\n",
        "    )\n",
        "\n",
        "    # pattern 1: ProperName (+ ProperName …) + geo suffix\n",
        "    pattern_suffix = re.compile(\n",
        "        rf\"\"\"\\b(?:[A-Z][\\w’'-]*(?:\\s+[A-Z][\\w’'-]*){{0,5}})\\s+{geo_suffixes}\\b\"\"\"\n",
        "    )\n",
        "\n",
        "    # pattern 2: Direction word + ProperName (e.g. \"West Africa\")\n",
        "    pattern_direction = re.compile(\n",
        "        r\"\"\"\\b(?:North|South|East|West|Northern|Southern|Eastern|Western)\\s+\"\"\"\n",
        "        r\"\"\"[A-Z][\\w’'-]+(?:\\s+[A-Z][\\w’'-]+){0,3}\\b\"\"\"\n",
        "    )\n",
        "\n",
        "    # pattern 3: ProperName + geo unit like Mine/Region/City\n",
        "    pattern_unit = re.compile(\n",
        "        r\"\"\"\\b(?:[A-Z][\\w’'-]+(?:\\s+[A-Z][\\w’'-]+){0,4})\\s+\"\"\"\n",
        "        r\"\"\"(?i:Mine|Mines|Goldmine|Goldfield|District|Province|Region|County|City|Town|Village)s?\\b\"\"\"\n",
        "    )\n",
        "\n",
        "    # quick quality check: phrase must look like a proper title\n",
        "    def looks_titlecase(phrase: str) -> bool:\n",
        "        phrase = re.sub(r\"\\s+\", \" \", phrase.strip())\n",
        "        tokens = phrase.split()\n",
        "        if not tokens:\n",
        "            return False\n",
        "        # single token: must start uppercase\n",
        "        if len(tokens) == 1:\n",
        "            return tokens[0][0].isupper()\n",
        "        # multiword: at least half tokens start uppercase\n",
        "        uppercase_count = sum(tok[0].isupper() for tok in tokens)\n",
        "        return uppercase_count >= max(2, len(tokens) // 2)\n",
        "\n",
        "    spans = []\n",
        "    for pattern in (pattern_suffix, pattern_direction, pattern_unit):\n",
        "        for match in pattern.finditer(text):\n",
        "            candidate = text[match.start():match.end()]\n",
        "            if looks_titlecase(candidate) and looks_like_location_phrase(candidate):\n",
        "                spans.append((match.start(), match.end(), \"regex\", \"GEO\", candidate))\n",
        "\n",
        "    return spans\n"
      ],
      "metadata": {
        "id": "sWTSWpkpHjuJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def regex_geo_extract_paragraphs(paragraphs):\n",
        "    \"\"\"\n",
        "    For each paragraph dict (filename, page, paragraph),\n",
        "    extract regex-matched locations.\n",
        "    Output: list of dicts: filename, page, mention, location, label (\"regex-GEO\")\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for item in paragraphs:\n",
        "        filename = item['filename']\n",
        "        page = item['page']\n",
        "        para = item['paragraph']\n",
        "        spans = regex_geo_spans(para)\n",
        "        for start, end, src, label, loc in spans:\n",
        "            results.append({\n",
        "                \"filename\": filename,\n",
        "                \"page\": page,\n",
        "                \"mention\": para,\n",
        "                \"location\": loc,\n",
        "                \"label\": \"regex-GEO\",\n",
        "                \"source\": \"regex\"\n",
        "            })\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "Hsabj89EK3QL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning, Deduplicating and Normalizing results from results of merging all 3 extractors."
      ],
      "metadata": {
        "id": "AzGncKczNmvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import io\n",
        "\n",
        "LEADING_DROP = (\n",
        "    r\"the\\b\", r\"preamble\\b\", r\"conclusion\\b\", r\"at\\b\"\n",
        ")\n",
        "\n",
        "# Compass modifiers like “NE-trending”, “NW-striking”, etc. to strip (case-insensitive)\n",
        "LEADING_COMPASS = r\"(?:\\b[NS][EW]?\\s*-(?:trending|striking)\\b\\s*)+\"\n",
        "\n",
        "# If any of these substrings appear, reject the candidate (case-insensitive)\n",
        "REJECT_SUBSTRINGS = (\n",
        "    \"declaration\", \"journal\", \"research \", \"figure\", \"table\",\n",
        "    \"universite\", \"university\", \"ministere\", \"ministry\",\n",
        "    \"geologists at\", \"capital city\", \"between the towns\",\n",
        "    \"various geological features\", \"in this region\"\n",
        ")\n",
        "\n",
        "# Single-word denials (exact, case-insensitive)\n",
        "REJECT_SINGLE = {\"harmattan\", \"north-east\", \"earth\", \"subcrop\"}\n",
        "\n",
        "# If present as a standalone token (case-sensitive), reject (common surnames/initials)\n",
        "REJECT_TOKENS = {\"Rogers\", \"Davis\", \"K.A.A\"}\n",
        "\n",
        "# Geo suffixes we consider meaningful for regex candidates\n",
        "GEO_SUFFIX_WORDS = {\n",
        "    \"Belt\",\"Greenstone Belt\",\"Craton\",\"Basin\",\"Shear Zone\",\"Fault\",\"Goldfield\",\n",
        "    \"Range\",\"Desert\",\"River\",\"Lake\",\"Sea\",\"Ocean\",\"Gulf\",\"Province\",\"State\",\n",
        "    \"Region\",\"Valley\",\"Plateau\",\"Peninsula\",\"Archipelago\",\"Canyon\",\"Strait\",\n",
        "    \"Channel\",\"Highlands\",\"Lowlands\",\"Orogeny\",\"Greenstone\",\"Mine\",\"Mines\",\n",
        "    \"Goldmine\",\"District\",\"County\",\"City\",\"Town\",\"Village\"\n",
        "}\n",
        "\n",
        "def _titlecase_if_allcaps(s: str) -> str:\n",
        "    # Keep acronyms with digits/short bits as-is; otherwise Title Case ALL-CAPS for display\n",
        "    if s.isupper() and len(s) > 3 and not re.search(r\"\\d\", s):\n",
        "        return \" \".join(w.capitalize() if len(w) > 1 else w for w in s.split())\n",
        "    return s\n",
        "\n",
        "def _strip_leading_noise(s: str) -> str:\n",
        "    t = s.strip()\n",
        "    # remove compass “NE-trending …” style sequences first\n",
        "    t = re.sub(rf\"^{LEADING_COMPASS}\", \"\", t, flags=re.IGNORECASE)\n",
        "    # strip determiners/section words like The/Preamble/Conclusion/At (possibly repeated)\n",
        "    changed = True\n",
        "    while changed:\n",
        "        changed = False\n",
        "        for key in LEADING_DROP:\n",
        "            new_t = re.sub(rf\"^{key}\\s+\", \"\", t, flags=re.IGNORECASE)\n",
        "            if new_t != t:\n",
        "                t = new_t\n",
        "                changed = True\n",
        "    return t.strip()\n",
        "\n",
        "def _has_geo_suffix(s: str) -> bool:\n",
        "    # true if ends with one of our geo suffix tokens (case-insensitive)\n",
        "    for suf in GEO_SUFFIX_WORDS:\n",
        "        if re.search(rf\"\\b{re.escape(suf)}\\b$\", s, flags=re.IGNORECASE):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def _looks_like_geo(s: str, source: str, label: str) -> bool:\n",
        "    low = s.lower()\n",
        "    if any(sub in low for sub in REJECT_SUBSTRINGS):\n",
        "        return False\n",
        "    toks = s.split()\n",
        "    if len(toks) == 1 and low in REJECT_SINGLE:\n",
        "        return False\n",
        "    if any(tok in REJECT_TOKENS for tok in toks):\n",
        "        return False\n",
        "    # For regex candidates, strongly prefer a geo suffix (“… Fault”, “… Basin”, “… Region”, …)\n",
        "    if source == \"regex\":\n",
        "        return _has_geo_suffix(s)\n",
        "    # For spaCy/GeoText, allow country/city/GPE tokens even without suffix\n",
        "    # but require Title-ish casing (not all-lowercase).\n",
        "    if s[0].islower():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def _norm_for_dedup_paragraph(loc: str) -> str:\n",
        "    s = re.sub(r\"\\s+\", \" \", loc.strip())\n",
        "    s = _strip_leading_noise(s)\n",
        "    return s.lower()\n",
        "\n",
        "def _clean_location_display_paragraph(s: str) -> str:\n",
        "    s = re.sub(r\"\\s+\", \" \", s.strip())\n",
        "    s = _strip_leading_noise(s)\n",
        "    s = _titlecase_if_allcaps(s)\n",
        "    return s"
      ],
      "metadata": {
        "id": "iVsdmMIlN06z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dedup_paragraph_location_results_from_lists(*results_lists):\n",
        "    \"\"\"\n",
        "    Accepts any number of result lists (e.g. spacy, geotext, regex), flattens, dedups, and returns\n",
        "    list of dicts: filename, page, mention, location, label, source.\n",
        "    \"\"\"\n",
        "    # Flatten all results into one list\n",
        "    all_results = []\n",
        "    for lst in results_lists:\n",
        "        all_results.extend(lst)\n",
        "\n",
        "    seen = set()\n",
        "    rows = []\n",
        "    for d in all_results:\n",
        "        fname = d['filename']\n",
        "        page = d['page']\n",
        "        para = d['mention']\n",
        "        loc = d['location']\n",
        "        label = d['label']\n",
        "        source = d.get('source', 'unknown')\n",
        "        loc_disp = _clean_location_display_paragraph(loc)\n",
        "        if not _looks_like_geo(loc_disp, source, label):\n",
        "            continue\n",
        "        key = (fname, _norm_for_dedup_paragraph(loc_disp))\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        rows.append({\n",
        "            \"filename\": fname,\n",
        "            \"page\": page,\n",
        "            \"mention\": para,\n",
        "            \"location\": loc_disp,\n",
        "            \"label\": label,\n",
        "            \"source\": source\n",
        "        })\n",
        "    return rows"
      ],
      "metadata": {
        "id": "bh4v4KXjRKca"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_txt_folder_to_csvs(folder_path, out_csv_folder, sentences_per_paragraph=6):\n",
        "    os.makedirs(out_csv_folder, exist_ok=True)\n",
        "    all_paragraphs = folder_txts_to_page_paragraphs(folder_path, sentences_per_paragraph)\n",
        "    # Group paragraphs by filename\n",
        "    file_to_paragraphs = defaultdict(list)\n",
        "    for p in all_paragraphs:\n",
        "        file_to_paragraphs[p[\"filename\"]].append(p)\n",
        "    for fname, paragraphs in file_to_paragraphs.items():\n",
        "        print(f\"Processing {fname}...\")\n",
        "        spacy_results = spacy_extract_locations(paragraphs)\n",
        "        geotext_results = geotext_extract_locations(paragraphs)\n",
        "        regex_results = regex_geo_extract_paragraphs(paragraphs)\n",
        "        deduped = dedup_paragraph_location_results_from_lists(\n",
        "            spacy_results, geotext_results, regex_results\n",
        "        )\n",
        "        out_csv = os.path.join(out_csv_folder, fname.replace(\".txt\", \"_locations.csv\"))\n",
        "        pd.DataFrame(deduped).to_csv(out_csv, index=False)\n",
        "        print(f\"Saved to {out_csv}, {len(deduped)} unique locations.\")\n"
      ],
      "metadata": {
        "id": "xCPvPiqzUgo6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_txt_folder_to_csvs(\"/content/drive/MyDrive/Data_txt\", \"/content/drive/MyDrive/Data4_rules_csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_5gJBsDUxJR",
        "outputId": "e453191b-20e5-42d2-8fe8-eb0f5be9acc8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 2013_FUNYUFUNYU.txt...\n",
            "Saved to /content/drive/MyDrive/Data4_rules_csv/2013_FUNYUFUNYU_locations.csv, 200 unique locations.\n",
            "Processing 2015_Masurel_phd.txt...\n",
            "Saved to /content/drive/MyDrive/Data4_rules_csv/2015_Masurel_phd_locations.csv, 319 unique locations.\n",
            "Processing 2013_Peters.txt...\n",
            "Saved to /content/drive/MyDrive/Data4_rules_csv/2013_Peters_locations.csv, 125 unique locations.\n",
            "Processing 2014_MSc_YOSSI.txt...\n",
            "Saved to /content/drive/MyDrive/Data4_rules_csv/2014_MSc_YOSSI_locations.csv, 109 unique locations.\n",
            "Processing 2009_Bontle Nkuna_0605886P_Honours Report.txt...\n",
            "Saved to /content/drive/MyDrive/Data4_rules_csv/2009_Bontle Nkuna_0605886P_Honours Report_locations.csv, 37 unique locations.\n",
            "Processing 2010_Mohale_GIS interpretation of NE Burkina Faso.txt...\n",
            "Saved to /content/drive/MyDrive/Data4_rules_csv/2010_Mohale_GIS interpretation of NE Burkina Faso_locations.csv, 51 unique locations.\n",
            "Processing 2007_Tshibubudze_THE MARKOYE FAULT_2007.txt...\n",
            "Saved to /content/drive/MyDrive/Data4_rules_csv/2007_Tshibubudze_THE MARKOYE FAULT_2007_locations.csv, 42 unique locations.\n",
            "Processing 2013_Ramabulana_Sadiola Hill petrology.txt...\n",
            "Saved to /content/drive/MyDrive/Data4_rules_csv/2013_Ramabulana_Sadiola Hill petrology_locations.csv, 46 unique locations.\n",
            "Processing 2011_Peters_East Markoye_2011.txt...\n",
            "Saved to /content/drive/MyDrive/Data4_rules_csv/2011_Peters_East Markoye_2011_locations.csv, 98 unique locations.\n",
            "Processing 2010_Matsheka_Irvin Final Thesis.txt...\n",
            "Saved to /content/drive/MyDrive/Data4_rules_csv/2010_Matsheka_Irvin Final Thesis_locations.csv, 41 unique locations.\n",
            "Processing 2008_MATABANE_FE3.txt...\n",
            "Saved to /content/drive/MyDrive/Data4_rules_csv/2008_MATABANE_FE3_locations.csv, 62 unique locations.\n",
            "Processing 2015_LeBrun_Siguiri.txt...\n",
            "Saved to /content/drive/MyDrive/Data4_rules_csv/2015_LeBrun_Siguiri_locations.csv, 255 unique locations.\n",
            "Processing 2012_Simoko_Petrology, geochemistry and structure of the Pissila batholith and the Saaba Zone gneiss.txt...\n",
            "Saved to /content/drive/MyDrive/Data4_rules_csv/2012_Simoko_Petrology, geochemistry and structure of the Pissila batholith and the Saaba Zone gneiss_locations.csv, 29 unique locations.\n",
            "Processing 2011_Woolfe_The stratigraphy and metamorphic facies of the KEMB.txt...\n",
            "Saved to /content/drive/MyDrive/Data4_rules_csv/2011_Woolfe_The stratigraphy and metamorphic facies of the KEMB_locations.csv, 24 unique locations.\n"
          ]
        }
      ]
    }
  ]
}