{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7639d13d",
   "metadata": {},
   "source": [
    "# Combined Geo Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbb3e83-4ff2-40e5-8d94-4a5cf54c47c6",
   "metadata": {},
   "source": [
    "The notebook automates the extraction, cleaning, geocoding, and mapping of location data from academic theses. It first filters out non-content sections like cover pages and references before converting the main body of each PDF into text with preserved page mapping. Locations are then identified using three extractors—spaCy for named entities, GeoText for cities and countries, and regex for GPS-style coordinates—and cleaned to remove generic terms and author names. Paragraphs are passed through a few-shot LLM to refine results, after which only LLM and LLM-plus-rules outputs are kept for geocoding. The final geocoding stage combines GeoPy with custom coordinate parsers that handle decimal, DMS, and UTM formats, adding latitude, longitude, and geocode type to each record while marking unmatched items as “no-geocode-result.” Duplicate localities are merged to retain all mentions and pages, and an interactive Folium map visualizes the results with popups showing each location’s context, providing an efficient pipeline for transforming unstructured text into verifiable geospatial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "774573cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import ast\n",
    "import io\n",
    "import csv\n",
    "from typing import List, Tuple\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from geotext import GeoText\n",
    "import unidecode\n",
    "from io import StringIO\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import folium\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyproj import Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda7676f-12f0-4172-8725-19936c2932bd",
   "metadata": {},
   "source": [
    "## Extract Data from PDF\n",
    "\n",
    "This section handles PDF text extraction and preprocessing. It filters out non-relevant sections such as cover pages, acknowledgements, references, and converts the main body into text with page mapping for later location lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c4cf6c-69a3-4e0c-b848-c260e73a1b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pages_text(pdf_path):\n",
    "    \"\"\" Input: PDF files\n",
    "        Output: list of pages as text(strings)\n",
    "    \"\"\"\n",
    "    pages = []\n",
    "    for page_layout in extract_pages(pdf_path):\n",
    "        lines = []\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                lines.append(element.get_text())\n",
    "        page_text = '\\n'.join(lines)\n",
    "        pages.append(page_text)\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e92984-40e9-471f-86d7-4c1d993a822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_toc_page(text):\n",
    "    if \"table of contents\" in text or \"contents\" in text:\n",
    "        return True\n",
    "    if re.search(r'\\.{5,}', text) and re.search(r'\\d{1,3}\\s*$', text, re.MULTILINE):\n",
    "        return True\n",
    "    if sum(1 for l in text.split('\\n') if re.match(r'.*\\d{1,3}\\s*$', l)) > 5:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_ack_page(text):\n",
    "    return \"acknowledgement\" in text or \"acknowledgments\" in text\n",
    "\n",
    "def is_declaration_page(text):\n",
    "    return \"declaration\" in text\n",
    "\n",
    "def is_main_section_start(text):\n",
    "    return bool(re.search(\n",
    "        r'\\b(?:1\\.|chapter\\s*1)[:\\s-]*introduction\\b|\\bintroduction\\b',\n",
    "        text, re.IGNORECASE\n",
    "    ))\n",
    "\n",
    "def remove_empty_lines(text):\n",
    "    return \"\\n\".join(line for line in text.splitlines() if line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb637734-dbfb-4eae-ab27-59764e2c7b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_references_sections(page_texts):\n",
    "    \"\"\"\n",
    "    Removes 'References' sections from each page and drops any blank pages.\n",
    "    Input: list of page_texts (strings)\n",
    "    Output: list of cleaned page_texts (strings)\n",
    "    \"\"\"\n",
    "    cleaned_pages = []\n",
    "    skip_mode = False\n",
    "    for page in page_texts:\n",
    "        lines = page.splitlines()\n",
    "        cleaned_lines = []\n",
    "        for line in lines:\n",
    "            # Detect references header\n",
    "            if not skip_mode and re.match(r'^\\s*(\\d+\\.?)?\\s*references\\b', line, re.I):\n",
    "                skip_mode = True\n",
    "                continue\n",
    "            # Exit skip mode if a new section/chapter starts\n",
    "            if skip_mode and (\n",
    "                re.match(r'^\\s*(chapter|paper|section|abstract|introduction)\\b', line, re.I) or\n",
    "                re.match(r'^\\s*(\\d+\\.?)?\\s*(abstract|introduction|chapter|paper|section)\\b', line, re.I)\n",
    "            ):\n",
    "                skip_mode = False\n",
    "            if not skip_mode:\n",
    "                cleaned_lines.append(line)\n",
    "        # Remove empty lines\n",
    "        non_empty = [l for l in cleaned_lines if l.strip()]\n",
    "        # If after cleaning, page is not blank, keep it\n",
    "        if non_empty:\n",
    "            cleaned_pages.append('\\n'.join(non_empty))\n",
    "    return cleaned_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8274b53-a313-42d8-98d7-64bd7b44b59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text_with_page_mapping(pdf_path):\n",
    "    pages = extract_pages_text(pdf_path)\n",
    "    body_pages = pages[1:]  # Removes cover page\n",
    "    filtered_pages = []\n",
    "    kept_pages = []\n",
    "    skip_mode = None\n",
    "    original_page_numbers = list(range(2, len(pages)+1))\n",
    "\n",
    "    for idx, pg in enumerate(body_pages):\n",
    "        pg_lower = pg.lower()\n",
    "        if skip_mode == 'toc':\n",
    "            if is_main_section_start(pg_lower):\n",
    "                skip_mode = None\n",
    "            elif is_toc_page(pg_lower):\n",
    "                continue\n",
    "            else:\n",
    "                skip_mode = None\n",
    "        elif skip_mode == 'ack':\n",
    "            if is_main_section_start(pg_lower):\n",
    "                skip_mode = None\n",
    "            elif is_ack_page(pg_lower):\n",
    "                continue\n",
    "            else:\n",
    "                skip_mode = None\n",
    "        elif skip_mode == 'dec':\n",
    "            if is_main_section_start(pg_lower):\n",
    "                skip_mode = None\n",
    "            elif is_declaration_page(pg_lower):\n",
    "                continue\n",
    "            else:\n",
    "                skip_mode = None\n",
    "        if skip_mode is None:\n",
    "            if is_toc_page(pg_lower):\n",
    "                skip_mode = 'toc'\n",
    "                continue\n",
    "            elif is_ack_page(pg_lower):\n",
    "                skip_mode = 'ack'\n",
    "                continue\n",
    "            elif is_declaration_page(pg_lower):\n",
    "                skip_mode = 'dec'\n",
    "                continue\n",
    "        filtered_pages.append(pg)\n",
    "        kept_pages.append(original_page_numbers[idx])\n",
    "\n",
    "    main_body_text = \"\\n\\n\".join(remove_empty_lines(pg) for pg in filtered_pages)\n",
    "\n",
    "    return main_body_text, kept_pages, filtered_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1041d52-ef30-454e-b2a3-8e7575b2156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_folder(folder_path, output_txt_folder=None, csv_report_path=None):\n",
    "    \"\"\"\n",
    "    Processes all PDFs in the given folder:\n",
    "    - Extracts text (and keeps track of page numbers).\n",
    "    - Removes all content from the References section onwards.\n",
    "    - Writes cleaned text to .txt files (with page markers).\n",
    "    - Prints and optionally saves a count per file as CSV.\n",
    "    \"\"\"\n",
    "    if output_txt_folder:\n",
    "        os.makedirs(output_txt_folder, exist_ok=True)\n",
    "    report = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            print(f\"Processing: {filename}\")\n",
    "            try:\n",
    "                main_text, kept_page_nums, kept_page_texts = pdf_to_text_with_page_mapping(pdf_path)\n",
    "                kept_page_texts_norefs = remove_references_sections(kept_page_texts)\n",
    "                kept_page_nums_norefs = kept_page_nums[:len(kept_page_texts_norefs)]\n",
    "\n",
    "                if output_txt_folder:\n",
    "                    txt_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "                    txt_path = os.path.join(output_txt_folder, txt_filename)\n",
    "                    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                        for page_num, page_text in zip(kept_page_nums_norefs, kept_page_texts_norefs):\n",
    "                            f.write(f\"\\n--- Page {page_num} ---\\n\")\n",
    "                            f.write(remove_empty_lines(page_text).strip() + \"\\n\")\n",
    "                report.append({\"filename\": filename, \"kept_pages\": len(kept_page_nums_norefs)})\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {filename}: {e}\")\n",
    "    print(\"\\n=== Page Count Report ===\")\n",
    "    for row in report:\n",
    "        print(f\"{row['filename']}: {row['kept_pages']} pages kept\")\n",
    "    if csv_report_path:\n",
    "        pd.DataFrame(report).to_csv(csv_report_path, index=False)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9d98bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 2007_Tshibubudze_THE MARKOYE FAULT_2007.pdf\n",
      "Processing: 2008_MATABANE_FE3.pdf\n",
      "Processing: 2009_Bontle Nkuna_0605886P_Honours Report.pdf\n",
      "Processing: 2010_Matsheka_Irvin Final Thesis.pdf\n",
      "Processing: 2010_Mohale_GIS interpretation of NE Burkina Faso.pdf\n",
      "Processing: 2011_Peters_East Markoye_2011.pdf\n",
      "Processing: 2011_Woolfe_The stratigraphy and metamorphic facies of the KEMB.pdf\n",
      "Processing: 2012_Simoko_Petrology, geochemistry and structure of the Pissila batholith and the Saaba Zone gneiss.pdf\n",
      "Processing: 2013_FUNYUFUNYU.pdf\n",
      "Processing: 2013_Peters.pdf\n",
      "Processing: 2013_Ramabulana_Sadiola Hill petrology.pdf\n",
      "Processing: 2014_MSc_YOSSI.pdf\n",
      "Processing: 2015_LeBrun_Siguiri.pdf\n",
      "Processing: 2015_Masurel_phd.pdf\n",
      "\n",
      "=== Page Count Report ===\n",
      "2007_Tshibubudze_THE MARKOYE FAULT_2007.pdf: 49 pages kept\n",
      "2008_MATABANE_FE3.pdf: 39 pages kept\n",
      "2009_Bontle Nkuna_0605886P_Honours Report.pdf: 45 pages kept\n",
      "2010_Matsheka_Irvin Final Thesis.pdf: 28 pages kept\n",
      "2010_Mohale_GIS interpretation of NE Burkina Faso.pdf: 41 pages kept\n",
      "2011_Peters_East Markoye_2011.pdf: 50 pages kept\n",
      "2011_Woolfe_The stratigraphy and metamorphic facies of the KEMB.pdf: 50 pages kept\n",
      "2012_Simoko_Petrology, geochemistry and structure of the Pissila batholith and the Saaba Zone gneiss.pdf: 32 pages kept\n",
      "2013_FUNYUFUNYU.pdf: 71 pages kept\n",
      "2013_Peters.pdf: 77 pages kept\n",
      "2013_Ramabulana_Sadiola Hill petrology.pdf: 34 pages kept\n",
      "2014_MSc_YOSSI.pdf: 38 pages kept\n",
      "2015_LeBrun_Siguiri.pdf: 191 pages kept\n",
      "2015_Masurel_phd.pdf: 227 pages kept\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    folder = \"DATA_FOR_MODELS/data_pdf\"\n",
    "    out_folder = \"DATA_FOR_MODELS/Data_txt\"\n",
    "    report_csv = \"DATA_FOR_MODELS/pdf_page_report.csv\"\n",
    "    process_pdf_folder(folder, out_folder, report_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bf7a393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sentence_paragraphs(text, sentences_per_paragraph=6):\n",
    "    # Basic sentence splitter\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    paragraphs = []\n",
    "    for i in range(0, len(sentences), sentences_per_paragraph):\n",
    "        para = \" \".join(sentences[i:i+sentences_per_paragraph])\n",
    "        paragraphs.append(para)\n",
    "    return paragraphs\n",
    "\n",
    "def txt_file_to_page_paragraphs(txt_path, sentences_per_paragraph=6):\n",
    "    \"\"\"\n",
    "    Reads a .txt file with \"--- Page N ---\" markers,\n",
    "    returns a list of dicts: {\"filename\", \"page\", \"paragraph\"}\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(txt_path)\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Split by page marker\n",
    "    page_blocks = re.split(r'\\n?--- Page (\\d+) ---\\n', text)\n",
    "    out = []\n",
    "    # page_blocks[0] is any text before the first page (often empty)\n",
    "    for i in range(1, len(page_blocks), 2):\n",
    "        page_num = int(page_blocks[i])\n",
    "        page_text = page_blocks[i+1]\n",
    "        paragraphs = text_to_sentence_paragraphs(page_text, sentences_per_paragraph)\n",
    "        for para in paragraphs:\n",
    "            if para.strip():\n",
    "                out.append({\n",
    "                    \"filename\": filename,\n",
    "                    \"page\": page_num,\n",
    "                    \"paragraph\": para.strip()\n",
    "                })\n",
    "    return out\n",
    "\n",
    "def folder_txts_to_page_paragraphs(folder_path, sentences_per_paragraph=6):\n",
    "    \"\"\"\n",
    "    For all .txt files in folder, returns a list of dicts:\n",
    "    filename, page, paragraph\n",
    "    \"\"\"\n",
    "    all_paragraphs = []\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.lower().endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, fname)\n",
    "            out = txt_file_to_page_paragraphs(file_path, sentences_per_paragraph)\n",
    "            all_paragraphs.extend(out)\n",
    "    return all_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07e514f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPASS = {\"N\",\"S\",\"E\",\"W\",\"NE\",\"NW\",\"SE\",\"SW\"}\n",
    "DENY_SINGLE = {\n",
    "    \"al.\", \"et\", \"al\", \"date\", \"university\", \"université\", \"ministere\", \"orpailleur\", \"brgm\",\n",
    "    \"thièblemont\", \"yacouba\"\n",
    "}\n",
    "DENY_TITLES = {\"university\", \"declaration\", \"preamble\", \"chapter\", \"figure\", \"table\"}\n",
    "DENY_PHRASE_PREFIXES = (\n",
    "    \"geologists at\", \"in this region\", \"various geological features\",\n",
    "    \"between the towns\", \"the region\", \"the desert\", \"the capital city\"\n",
    ")\n",
    "\n",
    "def looks_like_location_token(t: str) -> bool:\n",
    "    s = t.strip()\n",
    "    if not s: return False\n",
    "    if len(s) <= 2: return False\n",
    "    if s.upper() in COMPASS: return False\n",
    "    if s.lower() in DENY_SINGLE: return False\n",
    "    if s.endswith(\".\"): return False\n",
    "    if re.fullmatch(r\"[^\\w]+\", s): return False\n",
    "    return True\n",
    "\n",
    "def looks_like_location_phrase(t: str) -> bool:\n",
    "    s = re.sub(r\"\\s+\", \" \", t.strip())\n",
    "    if not looks_like_location_token(s):\n",
    "        return False\n",
    "    low = s.lower()\n",
    "    if low in DENY_TITLES: return False\n",
    "    if any(low.startswith(pref) for pref in DENY_PHRASE_PREFIXES): return False\n",
    "    if s.isupper() and \" \" not in s and not re.search(r\"[-’']\", s):\n",
    "        return False\n",
    "    words = s.split()\n",
    "    if len(words) >= 2:\n",
    "        titleish = sum(w[:1].isupper() for w in words) >= 1\n",
    "        if not titleish: return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f85f58-a0d7-4b10-91fc-bbb5a3054174",
   "metadata": {},
   "source": [
    "## Rules\n",
    "\n",
    "This section applies rule-based extraction methods including spaCy, GeoText, and regex patterns. It identifies cities, countries, coordinates, and region names from text, then cleans, deduplicates, and normalizes the results for merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd70cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_extract_locations(paragraphs):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    out = []\n",
    "    for item in paragraphs:\n",
    "        filename = item['filename']\n",
    "        page = item['page']\n",
    "        para = item['paragraph']\n",
    "        doc = nlp(para)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in {\"GPE\", \"LOC\", \"FAC\"}:\n",
    "                loc_text = ent.text.strip()\n",
    "                if looks_like_location_token(loc_text):\n",
    "                    out.append({\n",
    "                        \"filename\": filename,\n",
    "                        \"page\": page,\n",
    "                        \"mention\": para,\n",
    "                        \"location\": loc_text,\n",
    "                        \"label\": ent.label_,\n",
    "                        \"source\": \"spacy\"\n",
    "                    })\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d30f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geotext_extract_locations(paragraphs):\n",
    "    \"\"\"\n",
    "    For each paragraph dict (filename, page, paragraph),\n",
    "    extract city/country mentions using GeoText.\n",
    "    Output: list of dicts with filename, page, mention (the paragraph), location, and label (\"GEO\").\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for item in paragraphs:\n",
    "        filename = item['filename']\n",
    "        page = item['page']\n",
    "        para = item['paragraph']\n",
    "        places = GeoText(para)\n",
    "        # Combine all found locations\n",
    "        locs = set(list(places.cities) + list(places.countries))\n",
    "        for loc in locs:\n",
    "            if looks_like_location_token(loc):   # Use your custom filter!\n",
    "                out.append({\n",
    "                    \"filename\": filename,\n",
    "                    \"page\": page,\n",
    "                    \"mention\": para,\n",
    "                    \"location\": loc,\n",
    "                    \"label\": \"GEO\",\n",
    "                    \"source\": \"geotext\"\n",
    "                })\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "595ea18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_geo_spans(text: str) -> List[Tuple[int, int, str, str, str]]:\n",
    "    \"\"\"\n",
    "    Find geographic phrases using regex patterns.\n",
    "\n",
    "    Matches three main cases:\n",
    "      1. Proper name(s) followed by a geo suffix (e.g. \"Ashanti Belt\", \"West African Craton\")\n",
    "      2. Directional phrase + proper name (e.g. \"West Africa\", \"Eastern Highlands\")\n",
    "      3. Proper name(s) + geo unit (e.g. \"Essakane Mine\", \"Tarkwa Basin\", \"Pilbara Region\")\n",
    "\n",
    "    Returns a list of spans: (start_index, end_index, source, label, text)\n",
    "    \"\"\"\n",
    "\n",
    "    # case-insensitive suffixes that define a geo phrase\n",
    "    geo_suffixes = (\n",
    "        r\"(?i:\"  # (?i:) = case-insensitive group\n",
    "        r\"Belt|Greenstone Belt|Craton|Basin|Shear Zone|Fault|Goldfield|Range|Desert|River|\"\n",
    "        r\"Lake|Sea|Ocean|Gulf|Province|State|Region|Valley|Plateau|Peninsula|Archipelago|\"\n",
    "        r\"Canyon|Strait|Channel|Highlands|Lowlands|Orogeny|Greenstone\"\n",
    "        r\")\"\n",
    "    )\n",
    "\n",
    "    # pattern 1: ProperName (+ ProperName …) + geo suffix\n",
    "    pattern_suffix = re.compile(\n",
    "        rf\"\"\"\\b(?:[A-Z][\\w’'-]*(?:\\s+[A-Z][\\w’'-]*){{0,5}})\\s+{geo_suffixes}\\b\"\"\"\n",
    "    )\n",
    "\n",
    "    # pattern 2: Direction word + ProperName (e.g. \"West Africa\")\n",
    "    pattern_direction = re.compile(\n",
    "        r\"\"\"\\b(?:North|South|East|West|Northern|Southern|Eastern|Western)\\s+\"\"\"\n",
    "        r\"\"\"[A-Z][\\w’'-]+(?:\\s+[A-Z][\\w’'-]+){0,3}\\b\"\"\"\n",
    "    )\n",
    "\n",
    "    # pattern 3: ProperName + geo unit like Mine/Region/City\n",
    "    pattern_unit = re.compile(\n",
    "        r\"\"\"\\b(?:[A-Z][\\w’'-]+(?:\\s+[A-Z][\\w’'-]+){0,4})\\s+\"\"\"\n",
    "        r\"\"\"(?i:Mine|Mines|Goldmine|Goldfield|District|Province|Region|County|City|Town|Village)s?\\b\"\"\"\n",
    "    )\n",
    "\n",
    "    # quick quality check: phrase must look like a proper title\n",
    "    def looks_titlecase(phrase: str) -> bool:\n",
    "        phrase = re.sub(r\"\\s+\", \" \", phrase.strip())\n",
    "        tokens = phrase.split()\n",
    "        if not tokens:\n",
    "            return False\n",
    "        # single token: must start uppercase\n",
    "        if len(tokens) == 1:\n",
    "            return tokens[0][0].isupper()\n",
    "        # multiword: at least half tokens start uppercase\n",
    "        uppercase_count = sum(tok[0].isupper() for tok in tokens)\n",
    "        return uppercase_count >= max(2, len(tokens) // 2)\n",
    "\n",
    "    spans = []\n",
    "    for pattern in (pattern_suffix, pattern_direction, pattern_unit):\n",
    "        for match in pattern.finditer(text):\n",
    "            candidate = text[match.start():match.end()]\n",
    "            if looks_titlecase(candidate) and looks_like_location_phrase(candidate):\n",
    "                spans.append((match.start(), match.end(), \"regex\", \"GEO\", candidate))\n",
    "\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ff3b759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_geo_extract_paragraphs(paragraphs):\n",
    "    \"\"\"\n",
    "    For each paragraph dict (filename, page, paragraph),\n",
    "    extract regex-matched locations.\n",
    "    Output: list of dicts: filename, page, mention, location, label (\"regex-GEO\")\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for item in paragraphs:\n",
    "        filename = item['filename']\n",
    "        page = item['page']\n",
    "        para = item['paragraph']\n",
    "        spans = regex_geo_spans(para)\n",
    "        for start, end, src, label, loc in spans:\n",
    "            results.append({\n",
    "                \"filename\": filename,\n",
    "                \"page\": page,\n",
    "                \"mention\": para,\n",
    "                \"location\": loc,\n",
    "                \"label\": \"regex-GEO\",\n",
    "                \"source\": \"regex\"\n",
    "            })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "570a3adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEADING_DROP = (\n",
    "    r\"the\\b\", r\"preamble\\b\", r\"conclusion\\b\", r\"at\\b\"\n",
    ")\n",
    "\n",
    "# Compass modifiers like “NE-trending”, “NW-striking”, etc. to strip (case-insensitive)\n",
    "LEADING_COMPASS = r\"(?:\\b[NS][EW]?\\s*-(?:trending|striking)\\b\\s*)+\"\n",
    "\n",
    "# If any of these substrings appear, reject the candidate (case-insensitive)\n",
    "REJECT_SUBSTRINGS = (\n",
    "    \"declaration\", \"journal\", \"research \", \"figure\", \"table\",\n",
    "    \"universite\", \"university\", \"ministere\", \"ministry\",\n",
    "    \"geologists at\", \"capital city\", \"between the towns\",\n",
    "    \"various geological features\", \"in this region\"\n",
    ")\n",
    "\n",
    "# Single-word denials (exact, case-insensitive)\n",
    "REJECT_SINGLE = {\"harmattan\", \"north-east\", \"earth\", \"subcrop\"}\n",
    "\n",
    "# If present as a standalone token (case-sensitive), reject (common surnames/initials)\n",
    "REJECT_TOKENS = {\"Rogers\", \"Davis\", \"K.A.A\"}\n",
    "\n",
    "# Geo suffixes we consider meaningful for regex candidates\n",
    "GEO_SUFFIX_WORDS = {\n",
    "    \"Belt\",\"Greenstone Belt\",\"Craton\",\"Basin\",\"Shear Zone\",\"Fault\",\"Goldfield\",\n",
    "    \"Range\",\"Desert\",\"River\",\"Lake\",\"Sea\",\"Ocean\",\"Gulf\",\"Province\",\"State\",\n",
    "    \"Region\",\"Valley\",\"Plateau\",\"Peninsula\",\"Archipelago\",\"Canyon\",\"Strait\",\n",
    "    \"Channel\",\"Highlands\",\"Lowlands\",\"Orogeny\",\"Greenstone\",\"Mine\",\"Mines\",\n",
    "    \"Goldmine\",\"District\",\"County\",\"City\",\"Town\",\"Village\"\n",
    "}\n",
    "\n",
    "def _titlecase_if_allcaps(s: str) -> str:\n",
    "    # Keep acronyms with digits/short bits as-is; otherwise Title Case ALL-CAPS for display\n",
    "    if s.isupper() and len(s) > 3 and not re.search(r\"\\d\", s):\n",
    "        return \" \".join(w.capitalize() if len(w) > 1 else w for w in s.split())\n",
    "    return s\n",
    "\n",
    "def _strip_leading_noise(s: str) -> str:\n",
    "    t = s.strip()\n",
    "    # remove compass “NE-trending …” style sequences first\n",
    "    t = re.sub(rf\"^{LEADING_COMPASS}\", \"\", t, flags=re.IGNORECASE)\n",
    "    # strip determiners/section words like The/Preamble/Conclusion/At (possibly repeated)\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        for key in LEADING_DROP:\n",
    "            new_t = re.sub(rf\"^{key}\\s+\", \"\", t, flags=re.IGNORECASE)\n",
    "            if new_t != t:\n",
    "                t = new_t\n",
    "                changed = True\n",
    "    return t.strip()\n",
    "\n",
    "def _has_geo_suffix(s: str) -> bool:\n",
    "    # true if ends with one of our geo suffix tokens (case-insensitive)\n",
    "    for suf in GEO_SUFFIX_WORDS:\n",
    "        if re.search(rf\"\\b{re.escape(suf)}\\b$\", s, flags=re.IGNORECASE):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def _looks_like_geo(s: str, source: str, label: str) -> bool:\n",
    "    low = s.lower()\n",
    "    if any(sub in low for sub in REJECT_SUBSTRINGS):\n",
    "        return False\n",
    "    toks = s.split()\n",
    "    if len(toks) == 1 and low in REJECT_SINGLE:\n",
    "        return False\n",
    "    if any(tok in REJECT_TOKENS for tok in toks):\n",
    "        return False\n",
    "    # For regex candidates, strongly prefer a geo suffix (“… Fault”, “… Basin”, “… Region”, …)\n",
    "    if source == \"regex\":\n",
    "        return _has_geo_suffix(s)\n",
    "    # For spaCy/GeoText, allow country/city/GPE tokens even without suffix\n",
    "    # but require Title-ish casing (not all-lowercase).\n",
    "    if s[0].islower():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def _norm_for_dedup_paragraph(loc: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \" \", loc.strip())\n",
    "    s = _strip_leading_noise(s)\n",
    "    return s.lower()\n",
    "\n",
    "def _clean_location_display_paragraph(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \" \", s.strip())\n",
    "    s = _strip_leading_noise(s)\n",
    "    s = _titlecase_if_allcaps(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2fd8c8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup_paragraph_location_results_from_lists(*results_lists):\n",
    "    \"\"\"\n",
    "    Accepts any number of result lists (e.g. spacy, geotext, regex), flattens, dedups, and returns\n",
    "    list of dicts: filename, page, mention, location, label, source.\n",
    "    \"\"\"\n",
    "    # Flatten all results into one list\n",
    "    all_results = []\n",
    "    for lst in results_lists:\n",
    "        all_results.extend(lst)\n",
    "\n",
    "    seen = set()\n",
    "    rows = []\n",
    "    for d in all_results:\n",
    "        fname = d['filename']\n",
    "        page = d['page']\n",
    "        para = d['mention']\n",
    "        loc = d['location']\n",
    "        label = d['label']\n",
    "        source = d.get('source', 'unknown')\n",
    "        loc_disp = _clean_location_display_paragraph(loc)\n",
    "        if not _looks_like_geo(loc_disp, source, label):\n",
    "            continue\n",
    "        key = (fname, _norm_for_dedup_paragraph(loc_disp))\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        rows.append({\n",
    "            \"filename\": fname,\n",
    "            \"page\": page,\n",
    "            \"mention\": para,\n",
    "            \"location\": loc_disp,\n",
    "            \"label\": label,\n",
    "            \"source\": source\n",
    "        })\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d58ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_txt_folder_to_csvs(folder_path, out_csv_folder, sentences_per_paragraph=6):\n",
    "    os.makedirs(out_csv_folder, exist_ok=True)\n",
    "    all_paragraphs = folder_txts_to_page_paragraphs(folder_path, sentences_per_paragraph)\n",
    "    # Group paragraphs by filename\n",
    "    file_to_paragraphs = defaultdict(list)\n",
    "    for p in all_paragraphs:\n",
    "        file_to_paragraphs[p[\"filename\"]].append(p)\n",
    "    for fname, paragraphs in file_to_paragraphs.items():\n",
    "        print(f\"Processing {fname}...\")\n",
    "        spacy_results = spacy_extract_locations(paragraphs)\n",
    "        geotext_results = geotext_extract_locations(paragraphs)\n",
    "        regex_results = regex_geo_extract_paragraphs(paragraphs)\n",
    "        deduped = dedup_paragraph_location_results_from_lists(\n",
    "            spacy_results, geotext_results, regex_results\n",
    "        )\n",
    "        out_csv = os.path.join(out_csv_folder, fname.replace(\".txt\", \"_locations.csv\"))\n",
    "        pd.DataFrame(deduped).to_csv(out_csv, index=False)\n",
    "        print(f\"Saved to {out_csv}, {len(deduped)} unique locations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32284621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2007_Tshibubudze_THE MARKOYE FAULT_2007.txt...\n",
      "Saved to DATA_FOR_MODELS//Data4_rules_csv\\2007_Tshibubudze_THE MARKOYE FAULT_2007_locations.csv, 43 unique locations.\n",
      "Processing 2008_MATABANE_FE3.txt...\n",
      "Saved to DATA_FOR_MODELS//Data4_rules_csv\\2008_MATABANE_FE3_locations.csv, 76 unique locations.\n",
      "Processing 2009_Bontle Nkuna_0605886P_Honours Report.txt...\n",
      "Saved to DATA_FOR_MODELS//Data4_rules_csv\\2009_Bontle Nkuna_0605886P_Honours Report_locations.csv, 47 unique locations.\n",
      "Processing 2010_Matsheka_Irvin Final Thesis.txt...\n",
      "Saved to DATA_FOR_MODELS//Data4_rules_csv\\2010_Matsheka_Irvin Final Thesis_locations.csv, 51 unique locations.\n",
      "Processing 2010_Mohale_GIS interpretation of NE Burkina Faso.txt...\n",
      "Saved to DATA_FOR_MODELS//Data4_rules_csv\\2010_Mohale_GIS interpretation of NE Burkina Faso_locations.csv, 58 unique locations.\n",
      "Processing 2011_Peters_East Markoye_2011.txt...\n",
      "Saved to DATA_FOR_MODELS//Data4_rules_csv\\2011_Peters_East Markoye_2011_locations.csv, 99 unique locations.\n",
      "Processing 2011_Woolfe_The stratigraphy and metamorphic facies of the KEMB.txt...\n",
      "Saved to DATA_FOR_MODELS//Data4_rules_csv\\2011_Woolfe_The stratigraphy and metamorphic facies of the KEMB_locations.csv, 30 unique locations.\n",
      "Processing 2012_Simoko_Petrology, geochemistry and structure of the Pissila batholith and the Saaba Zone gneiss.txt...\n",
      "Saved to DATA_FOR_MODELS//Data4_rules_csv\\2012_Simoko_Petrology, geochemistry and structure of the Pissila batholith and the Saaba Zone gneiss_locations.csv, 38 unique locations.\n",
      "Processing 2013_FUNYUFUNYU.txt...\n",
      "Saved to DATA_FOR_MODELS//Data4_rules_csv\\2013_FUNYUFUNYU_locations.csv, 198 unique locations.\n",
      "Processing 2013_Peters.txt...\n",
      "Saved to DATA_FOR_MODELS//Data4_rules_csv\\2013_Peters_locations.csv, 131 unique locations.\n",
      "Processing 2013_Ramabulana_Sadiola Hill petrology.txt...\n",
      "Saved to DATA_FOR_MODELS//Data4_rules_csv\\2013_Ramabulana_Sadiola Hill petrology_locations.csv, 42 unique locations.\n",
      "Processing 2014_MSc_YOSSI.txt...\n",
      "Saved to DATA_FOR_MODELS//Data4_rules_csv\\2014_MSc_YOSSI_locations.csv, 127 unique locations.\n",
      "Processing 2015_LeBrun_Siguiri.txt...\n",
      "Saved to DATA_FOR_MODELS//Data4_rules_csv\\2015_LeBrun_Siguiri_locations.csv, 289 unique locations.\n",
      "Processing 2015_Masurel_phd.txt...\n",
      "Saved to DATA_FOR_MODELS//Data4_rules_csv\\2015_Masurel_phd_locations.csv, 341 unique locations.\n"
     ]
    }
   ],
   "source": [
    "process_txt_folder_to_csvs(\"DATA_FOR_MODELS//Data_txt\", \"DATA_FOR_MODELS//Data4_rules_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57fda71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sentence_paragraphs(text, sentences_per_paragraph=6):\n",
    "    # Basic sentence splitter\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    paragraphs = []\n",
    "    for i in range(0, len(sentences), sentences_per_paragraph):\n",
    "        para = \" \".join(sentences[i:i+sentences_per_paragraph])\n",
    "        paragraphs.append(para)\n",
    "    return paragraphs\n",
    "\n",
    "def txt_file_to_page_paragraphs(txt_path, sentences_per_paragraph=6):\n",
    "    \"\"\"\n",
    "    Reads a .txt file with \"--- Page N ---\" markers,\n",
    "    returns a list of dicts: {\"filename\", \"page\", \"paragraph\"}\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(txt_path)\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Split by page marker\n",
    "    page_blocks = re.split(r'\\n?--- Page (\\d+) ---\\n', text)\n",
    "    out = []\n",
    "    # page_blocks[0] is any text before the first page (often empty)\n",
    "    for i in range(1, len(page_blocks), 2):\n",
    "        page_num = int(page_blocks[i])\n",
    "        page_text = page_blocks[i+1]\n",
    "        paragraphs = text_to_sentence_paragraphs(page_text, sentences_per_paragraph)\n",
    "        for para in paragraphs:\n",
    "            if para.strip():\n",
    "                out.append({\n",
    "                    \"filename\": filename,\n",
    "                    \"page\": page_num,\n",
    "                    \"paragraph\": para.strip()\n",
    "                })\n",
    "    return out\n",
    "\n",
    "def folder_txts_to_page_paragraphs(folder_path, sentences_per_paragraph=6):\n",
    "    \"\"\"\n",
    "    For all .txt files in folder, returns a list of dicts:\n",
    "    filename, page, paragraph\n",
    "    \"\"\"\n",
    "    all_paragraphs = []\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.lower().endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, fname)\n",
    "            out = txt_file_to_page_paragraphs(file_path, sentences_per_paragraph)\n",
    "            all_paragraphs.extend(out)\n",
    "    return all_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4378c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"DATA_FOR_MODELS/Data_txt\"\n",
    "paragraphs = folder_txts_to_page_paragraphs(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef52b523-941c-4b19-b3ad-b0a12b0324b3",
   "metadata": {},
   "source": [
    "## LLM\n",
    "\n",
    "This section generates paragraph inputs for large language model (LLM) extraction using few-shot prompts. The LLM improves accuracy by resolving ambiguous mentions and contextualizing geological localities. Only LLM and LLM+rules data are retained for final geocoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "83240a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API connection successful! Number of models: 93\n"
     ]
    }
   ],
   "source": [
    "# API key, replace with yours\n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "\n",
    "# Testing API connection\n",
    "client = OpenAI()\n",
    "try:\n",
    "    model_list = client.models.list()\n",
    "    print(\"API connection successful! Number of models:\", len(model_list.data))\n",
    "except Exception as e:\n",
    "    print(\"API connection failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "29d5c67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  FEW SHOT PROMPT FOR GEO LOCALITY EXTRACTION\n",
    "few_shot_prompts = \"\"\"\n",
    "You are an assistant that extracts locality descriptions from geoscience texts.\n",
    "A locality description can be:\n",
    "- a site, region, province, district, city, towns, villages, or a GPS coordinate (e.g., UTM, lat/lon).\n",
    "- Do NOT generate or invent localities. Only extract those *explicitly mentioned* in the paragraph.\n",
    "- If there are no localities mentioned, output: []\n",
    "\n",
    "For each paragraph, extract the *relevant locality descriptions* as a list. If none, output: None.\n",
    "Example 1:\n",
    "Paragraph: The NE-trending Markoye fault in Burkina Faso is a first-order crustal scale structure located in the north-eastern part of Burkina Faso,between the towns of Dori and Tambão by the border with Mali and Niger.\n",
    "Localities: [\"north-eastern part of Burkina Faso\", \"towns of Dori and Tambão\", \"border with Mali and Niger\"]\n",
    "\n",
    "Example 2:\n",
    "Paragraph: Several east-west traverses were conducted across the Markoye fault between Essakane and Tambão. Field stations were established at the villages of Salmossi, Essakane, Tin Agadel, Tin Taradat, Markoye and Gorom-Gorom.\n",
    "Localities: [\"Markoye fault\", \"Essakane\", \"Tambão\", \"Salmossi\", \"Tin Agadel\", \"Tin Taradat\", \"Markoye\", \"Gorom-Gorom\"]\n",
    "\n",
    "Example 3:\n",
    "Paragraph: The area of study is located ±250 km NE of the capital city, Ouagadougou, Burkina Faso (Fig. 1). The extent of the study area is demarcated by the villages of Essakane to the south-west, Tin Agadel and Tin Taradat to the north-east, and Markoye and Gorom-Gorom to the east.\n",
    "Localities: [\"250 km NE of Ouagadougou, Burkina Faso\", \"Essakane\", \"Tin Agadel\", \"Tin Taradat\", \"Markoye\", \"Gorom-Gorom\"]\n",
    "\n",
    "Example 4:\n",
    "Paragraph: The UTM coordinates of the field area are: 30 P 0805000 E, 1600000 N; 0805000 E, 1594000 N; 0810000 E, 1594000 N; 0810000 E, 1600000 N.\n",
    "Localities: [\"30 P 0805000 E, 1600000 N\", \"0805000 E, 1594000 N\", \"0810000 E, 1594000 N\", \"0810000 E, 1600000 N\"]\n",
    "\n",
    "Example 5:\n",
    "Paragraph: No specific localities or coordinates are mentioned in this paragraph.\n",
    "Localities: []\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d612b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_localities_llm(paragraph, model=\"gpt-4o\"):\n",
    "    prompt = (\n",
    "        few_shot_prompts +\n",
    "        f\"\\nParagraph: {paragraph}\\nLocalities:\"\n",
    "    )\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=120,\n",
    "        temperature=0,\n",
    "    )\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    # Try to extract the list\n",
    "    m = re.search(r\"\\[(.*?)\\]\", answer, re.DOTALL)\n",
    "    if m:\n",
    "        raw_list = m.group(0)\n",
    "        try:\n",
    "            extracted = ast.literal_eval(raw_list)\n",
    "            if isinstance(extracted, str):  # Occasionally LLM returns string not list\n",
    "                extracted = [extracted]\n",
    "        except:\n",
    "            extracted = []\n",
    "    else:\n",
    "        extracted = []\n",
    "    return extracted\n",
    "\n",
    "def extract_from_paragraphs_w_page(paragraphs, model=\"gpt-4o\"):\n",
    "    if not paragraphs:\n",
    "        print(\"No paragraphs provided.\")\n",
    "        return []\n",
    "    print(f\"\\nProcessing: {paragraphs[0]['filename']} ({len(paragraphs)} paragraphs)\")\n",
    "    results = []\n",
    "    for p in paragraphs:\n",
    "        para = p['paragraph']\n",
    "        page = p['page']\n",
    "        filename = p['filename']\n",
    "        if len(para.strip()) < 10:\n",
    "            continue\n",
    "        retry = 0\n",
    "        while True:\n",
    "            try:\n",
    "                localities = extract_localities_llm(para, model=model)\n",
    "                if localities:\n",
    "                    results.append({\n",
    "                        \"filename\": filename,\n",
    "                        \"page\": page,\n",
    "                        \"mention\": para,\n",
    "                        \"location\": localities\n",
    "                    })\n",
    "                break\n",
    "            except openai.RateLimitError:\n",
    "                print(\"Rate limit hit, waiting 3 seconds...\")\n",
    "                time.sleep(3)\n",
    "                retry += 1\n",
    "                if retry > 5:\n",
    "                    print(\"Too many retries, skipping this paragraph.\")\n",
    "                    break\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "72b230ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already processed 2007_Tshibubudze_THE MARKOYE FAULT_2007.txt, skipping.\n",
      "Already processed 2008_MATABANE_FE3.txt, skipping.\n",
      "Already processed 2009_Bontle Nkuna_0605886P_Honours Report.txt, skipping.\n",
      "Already processed 2010_Matsheka_Irvin Final Thesis.txt, skipping.\n",
      "Already processed 2010_Mohale_GIS interpretation of NE Burkina Faso.txt, skipping.\n",
      "Already processed 2011_Peters_East Markoye_2011.txt, skipping.\n",
      "Already processed 2011_Woolfe_The stratigraphy and metamorphic facies of the KEMB.txt, skipping.\n",
      "Already processed 2012_Simoko_Petrology, geochemistry and structure of the Pissila batholith and the Saaba Zone gneiss.txt, skipping.\n",
      "Already processed 2013_FUNYUFUNYU.txt, skipping.\n",
      "Already processed 2013_Peters.txt, skipping.\n",
      "Already processed 2013_Ramabulana_Sadiola Hill petrology.txt, skipping.\n",
      "Already processed 2014_MSc_YOSSI.txt, skipping.\n",
      "Already processed 2015_LeBrun_Siguiri.txt, skipping.\n",
      "Already processed 2015_Masurel_phd.txt, skipping.\n"
     ]
    }
   ],
   "source": [
    "def batch_extract_localities_llm_paragraphs(paragraphs, output_folder, model=\"gpt-4o\"):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    # Group paragraphs by filename\n",
    "    by_file = defaultdict(list)\n",
    "    for p in paragraphs:\n",
    "        by_file[p['filename']].append(p)\n",
    "\n",
    "    for fname, plist in by_file.items():\n",
    "        out_csv = os.path.join(output_folder, fname.replace(\".txt\", \"_locations_llm.csv\"))\n",
    "        if os.path.exists(out_csv):\n",
    "            print(f\"Already processed {fname}, skipping.\")\n",
    "            continue\n",
    "        print(f\"Extracting for {fname} ({len(plist)} paragraphs)\")\n",
    "        results = []\n",
    "        for p in plist:\n",
    "            para = p['paragraph']\n",
    "            page = p['page']\n",
    "            retry = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    locs = extract_localities_llm(para, model=model)\n",
    "                    if locs:\n",
    "                        for loc in locs:\n",
    "                            results.append({\n",
    "                                \"filename\": fname,\n",
    "                                \"page\": page,\n",
    "                                \"mention\": para,\n",
    "                                \"location\": loc\n",
    "                            })\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}. Sleeping...\")\n",
    "                    time.sleep(4)\n",
    "                    retry += 1\n",
    "                    if retry > 5:\n",
    "                        print(\"Failed after many retries, skipping paragraph.\")\n",
    "                        break\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_csv(out_csv, index=False)\n",
    "        print(f\"Saved {len(df)} locations to {out_csv}\")\n",
    "\n",
    "\n",
    "paragraphs = folder_txts_to_page_paragraphs(\"DATA_FOR_MODELS/Data_txt\", sentences_per_paragraph=6)\n",
    "batch_extract_localities_llm_paragraphs(paragraphs, \"DATA_FOR_MODELS/Data_csv_llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8aa3d7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved deduped: DATA_FOR_MODELS/Data_LLM_CSV_clean\\2007_Tshibubudze_THE MARKOYE FAULT_2007_locations_llm_clean.csv\n",
      "Saved deduped: DATA_FOR_MODELS/Data_LLM_CSV_clean\\2008_MATABANE_FE3_locations_llm_clean.csv\n",
      "Saved deduped: DATA_FOR_MODELS/Data_LLM_CSV_clean\\2009_Bontle Nkuna_0605886P_Honours Report_locations_llm_clean.csv\n",
      "Saved deduped: DATA_FOR_MODELS/Data_LLM_CSV_clean\\2010_Matsheka_Irvin Final Thesis_locations_llm_clean.csv\n",
      "Saved deduped: DATA_FOR_MODELS/Data_LLM_CSV_clean\\2010_Mohale_GIS interpretation of NE Burkina Faso_locations_llm_clean.csv\n",
      "Saved deduped: DATA_FOR_MODELS/Data_LLM_CSV_clean\\2011_Peters_East Markoye_2011_locations_llm_clean.csv\n",
      "Saved deduped: DATA_FOR_MODELS/Data_LLM_CSV_clean\\2011_Woolfe_The stratigraphy and metamorphic facies of the KEMB_locations_llm_clean.csv\n",
      "Saved deduped: DATA_FOR_MODELS/Data_LLM_CSV_clean\\2012_Simoko_Petrology, geochemistry and structure of the Pissila batholith and the Saaba Zone gneiss_locations_llm_clean.csv\n",
      "Saved deduped: DATA_FOR_MODELS/Data_LLM_CSV_clean\\2013_FUNYUFUNYU_locations_llm_clean.csv\n",
      "Saved deduped: DATA_FOR_MODELS/Data_LLM_CSV_clean\\2013_Peters_locations_llm_clean.csv\n",
      "Saved deduped: DATA_FOR_MODELS/Data_LLM_CSV_clean\\2013_Ramabulana_Sadiola Hill petrology_locations_llm_clean.csv\n",
      "Saved deduped: DATA_FOR_MODELS/Data_LLM_CSV_clean\\2014_MSc_YOSSI_locations_llm_clean.csv\n",
      "Saved deduped: DATA_FOR_MODELS/Data_LLM_CSV_clean\\2015_LeBrun_Siguiri_locations_llm_clean.csv\n",
      "Saved deduped: DATA_FOR_MODELS/Data_LLM_CSV_clean\\2015_Masurel_phd_locations_llm_clean.csv\n"
     ]
    }
   ],
   "source": [
    "def clean_location(loc):\n",
    "    if pd.isna(loc):\n",
    "        return \"\"\n",
    "    return unidecode.unidecode(str(loc).strip())\n",
    "\n",
    "def agg_pages(series):\n",
    "    return ','.join(sorted({str(x) for x in series if pd.notna(x)}))\n",
    "\n",
    "def agg_mentions(series):\n",
    "    return \" || \".join(sorted({str(x).strip() for x in series if pd.notna(x)}))\n",
    "\n",
    "def deduplicate_llm_csv(input_csv, output_csv):\n",
    "    df = pd.read_csv(input_csv)\n",
    "    if 'location' not in df.columns:\n",
    "        print(f\"Skipped {input_csv} (no 'location' column)\")\n",
    "        return\n",
    "    df['location'] = df['location'].map(clean_location)\n",
    "    df_dedup = (\n",
    "        df.groupby(['filename', 'location'])\n",
    "        .agg({\n",
    "            'page': agg_pages,\n",
    "            'mention': agg_mentions\n",
    "        })\n",
    "        .reset_index()\n",
    "        .rename(columns={'page': 'page', 'mention': 'mention'})\n",
    "    )\n",
    "    df_dedup = df_dedup[['filename','page', 'mention','location']]\n",
    "    df_dedup['source']=\"llm\"\n",
    "    df_dedup.to_csv(output_csv, index=False)\n",
    "    print(f\"Saved deduped: {output_csv}\")\n",
    "\n",
    "def batch_clean_locations(input_folder, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    for fname in os.listdir(input_folder):\n",
    "        if fname.lower().endswith(\".csv\"):\n",
    "            inpath = os.path.join(input_folder, fname)\n",
    "            outname = os.path.splitext(fname)[0] + \"_clean.csv\"\n",
    "            outpath = os.path.join(output_folder, outname)\n",
    "            try:\n",
    "                deduplicate_llm_csv(inpath, outpath)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {fname}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "input_folder = \"DATA_FOR_MODELS/Data_csv_llm\"\n",
    "output_folder = \"DATA_FOR_MODELS/Data_LLM_CSV_clean\"\n",
    "\n",
    "batch_clean_locations(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b054a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_pages(series):\n",
    "    return ','.join(sorted({str(x) for x in series if pd.notna(x)}))\n",
    "\n",
    "def agg_mentions(series):\n",
    "    return \" || \".join(sorted({str(x).strip() for x in series if pd.notna(x)}))\n",
    "\n",
    "def get_base_key(fname):\n",
    "    # for match\n",
    "    return (fname.lower()\n",
    "            .replace('_locations.csv', '')\n",
    "            .replace('_locations_llm_clean.csv', '')\n",
    "            .replace('.csv', '')\n",
    "            .replace(' ', '')\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "162d652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_rule_llm_pair(rules_csv, llm_csv, out_csv):\n",
    "\n",
    "    # RULES\n",
    "    df_rules = pd.read_csv(rules_csv)\n",
    "    if not {'filename', 'page', 'mention', 'location'}.issubset(df_rules.columns):\n",
    "        print(f\"Skipping {rules_csv}: missing required columns\")\n",
    "        return\n",
    "    # Label as rules\n",
    "    df_rules = df_rules[['filename', 'page', 'mention', 'location']].copy()\n",
    "    df_rules[\"source\"] = \"rules\"\n",
    "\n",
    "    # LLM\n",
    "    df_llm = pd.read_csv(llm_csv)\n",
    "    if not {'filename', 'page', 'mention', 'location'}.issubset(df_llm.columns):\n",
    "        print(f\"Skipping {llm_csv}: missing required columns\")\n",
    "        return\n",
    "    df_llm = df_llm[['filename', 'page', 'mention', 'location']].copy()\n",
    "    df_llm[\"source\"] = \"llm\"\n",
    "\n",
    "    # Combine\n",
    "    df_all = pd.concat([df_rules, df_llm], ignore_index=True)\n",
    "\n",
    "    # Group and Merge Sources\n",
    "    df_merged = (\n",
    "        df_all\n",
    "        .groupby(['filename', 'location'])\n",
    "        .agg({\n",
    "            'page': agg_pages,\n",
    "            'mention': agg_mentions,\n",
    "            'source': lambda s: ','.join(sorted(set(s))),\n",
    "        })\n",
    "        .reset_index()\n",
    "        .rename(columns={'page': 'pages', 'mention': 'mentions'})\n",
    "    )\n",
    "\n",
    "\n",
    "    df_merged.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved merged file: {out_csv}\")\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d3a80f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_rule_llm_files(rules_folder, llm_folder, out_folder):\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    rules_files = {get_base_key(f): f for f in os.listdir(rules_folder) if f.endswith(\"_locations.csv\")}\n",
    "    llm_files = {get_base_key(f): f for f in os.listdir(llm_folder) if \"_llm_clean.csv\" in f.lower()}\n",
    "    matched = rules_files.keys() & llm_files.keys()\n",
    "    print(f\"RULES base keys: {list(rules_files.keys())}\")\n",
    "    print(f\"LLM base keys: {list(llm_files.keys())}\")\n",
    "    print(f\"Found {len(matched)} matching base keys to merge.\")\n",
    "    for key in matched:\n",
    "        rules_path = os.path.join(rules_folder, rules_files[key])\n",
    "        llm_path = os.path.join(llm_folder, llm_files[key])\n",
    "        out_path = os.path.join(out_folder, f\"{key}_merged.csv\")\n",
    "        merge_rule_llm_pair(rules_path, llm_path, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e3875f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RULES base keys: ['2007_tshibubudze_themarkoyefault_2007', '2008_matabane_fe3', '2009_bontlenkuna_0605886p_honoursreport', '2010_matsheka_irvinfinalthesis', '2010_mohale_gisinterpretationofneburkinafaso', '2011_peters_eastmarkoye_2011', '2011_woolfe_thestratigraphyandmetamorphicfaciesofthekemb', '2012_simoko_petrology,geochemistryandstructureofthepissilabatholithandthesaabazonegneiss', '2013_funyufunyu', '2013_peters', '2013_ramabulana_sadiolahillpetrology', '2014_msc_yossi', '2015_lebrun_siguiri', '2015_masurel_phd']\n",
      "LLM base keys: ['2007_tshibubudze_themarkoyefault_2007', '2008_matabane_fe3', '2009_bontlenkuna_0605886p_honoursreport', '2010_matsheka_irvinfinalthesis', '2010_mohale_gisinterpretationofneburkinafaso', '2011_peters_eastmarkoye_2011', '2011_woolfe_thestratigraphyandmetamorphicfaciesofthekemb', '2012_simoko_petrology,geochemistryandstructureofthepissilabatholithandthesaabazonegneiss', '2013_funyufunyu', '2013_peters', '2013_ramabulana_sadiolahillpetrology', '2014_msc_yossi', '2015_lebrun_siguiri', '2015_masurel_phd']\n",
      "Found 14 matching base keys to merge.\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2011_peters_eastmarkoye_2011_merged.csv\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2009_bontlenkuna_0605886p_honoursreport_merged.csv\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2013_peters_merged.csv\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2007_tshibubudze_themarkoyefault_2007_merged.csv\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2014_msc_yossi_merged.csv\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2008_matabane_fe3_merged.csv\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2012_simoko_petrology,geochemistryandstructureofthepissilabatholithandthesaabazonegneiss_merged.csv\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2010_mohale_gisinterpretationofneburkinafaso_merged.csv\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2011_woolfe_thestratigraphyandmetamorphicfaciesofthekemb_merged.csv\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2015_masurel_phd_merged.csv\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2013_ramabulana_sadiolahillpetrology_merged.csv\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2013_funyufunyu_merged.csv\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2015_lebrun_siguiri_merged.csv\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2010_matsheka_irvinfinalthesis_merged.csv\n"
     ]
    }
   ],
   "source": [
    "merge_all_rule_llm_files(\"DATA_FOR_MODELS/Data4_rules_csv\", \"DATA_FOR_MODELS/Data_LLM_CSV_clean\", \"DATA_FOR_MODELS/Data4_merged2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b0e351b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_citation(mention, loc):\n",
    "    # Looks for patterns like \"Ledru et al.\", \"(Taylor, 1990)\", etc.\n",
    "    return bool(re.search(rf\"\\b{re.escape(loc)}\\b.*et al\\.?\", mention, re.IGNORECASE)) or \\\n",
    "           bool(re.search(rf\"\\({re.escape(loc)}, \\d{{4}}\\)\", mention))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "96fd562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def is_person_entity(loc):\n",
    "    doc = nlp(loc)\n",
    "    return any(ent.label_ == \"PERSON\" for ent in doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "191d2f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_author_like(loc, mention):\n",
    "    loc_clean = loc.strip()\n",
    "    if is_citation(mention, loc_clean):\n",
    "        return True\n",
    "    if is_person_entity(loc_clean):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17631d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_out_authors(df):\n",
    "    # Expects 'location' and 'mentions' columns\n",
    "    mask = df.apply(lambda row: not is_author_like(row['location'], row['mentions']), axis=1)\n",
    "    return df[mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cd956547",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERIC_LOCATION_WORDS = [\n",
    "    \"study area\", \"area of study\", \"area of investigation\", \"study region\", \"study site\",\n",
    "    \"study location\", \"area\", \"region\", \"zone\", \"locality\", \"localities\",\n",
    "    \"the area\", \"the region\", \"investigation area\", \"site\", \"sites\"\n",
    "]\n",
    "\n",
    "def remove_generic_location_words(location):\n",
    "    \"\"\"\n",
    "    Removes generic location words/phrases (case-insensitive) from start or end of a location string.\n",
    "    \"\"\"\n",
    "    if not isinstance(location, str):\n",
    "        return location\n",
    "    s = location.strip()\n",
    "    # Build a regex to match generic words at start or end (with optional spaces/punctuation)\n",
    "    pattern = r\"^(%s)\\b[\\s,:\\-]*|[\\s,:\\-]*(%s)$\" % (\n",
    "        \"|\".join(map(re.escape, GENERIC_LOCATION_WORDS)),\n",
    "        \"|\".join(map(re.escape, GENERIC_LOCATION_WORDS))\n",
    "    )\n",
    "    s = re.sub(pattern, \"\", s, flags=re.IGNORECASE)\n",
    "    # Remove double spaces, commas, etc.\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = s.strip(\",.:-; \")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aad1dc8-544c-4fe6-915b-26821f767c4d",
   "metadata": {},
   "source": [
    "## Geocoding and Mapping\n",
    "\n",
    "This section parses coordinates (decimal, DMS, UTM), performs geocoding using GeoPy, and visualizes results on an interactive Folium map. It adds latitude, longitude, accuracy, and type columns while removing rule-only rows. Maps display each location’s mentions and page numbers for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "49bed75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_gps(s):\n",
    "    # Simple patterns for decimal/sexagesimal degrees or UTM\n",
    "    s = str(s).strip()\n",
    "    # Simple decimal degree/UTM/Easting-Northing patterns:\n",
    "    if re.match(r\"^\\d{5,}[-; ]\\s*\\d{5,}$\", s):  # e.g., 0184836-1587581 or 1534836; 0216256\n",
    "        return True\n",
    "    # Matches \"N 14 36 37 8 E 00 00 12 1\" etc.\n",
    "    if re.search(r\"[NSEW]\\s*\\d+\", s) and re.search(r\"[EW]\\s*\\d+\", s):\n",
    "        return True\n",
    "    # Matches \"GPS\" in string (sometimes extracted as \"GPS 14 35 05 7 W 00 00 05 4\")\n",
    "    if s.upper().startswith(\"GPS\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_approximate(loc):\n",
    "    if pd.isna(loc): return False\n",
    "    loc = str(loc).strip()\n",
    "    # Match \"NE of Burkina Faso\", \"southern part of Ghana\", etc.\n",
    "    direction = r\"(?:N|S|E|W|NE|NW|SE|SW|North|South|East|West|Northern|Southern|Eastern|Western)\"\n",
    "    if re.match(rf\"^{direction}(\\s+part)?\\s+of\\s+.+\", loc, flags=re.IGNORECASE):\n",
    "        return True\n",
    "    if re.match(rf\"^.+\\b({direction})\\b\", loc, flags=re.IGNORECASE):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_geounit(s):\n",
    "    geo_unit_keywords = [\n",
    "        \"belt\", \"craton\", \"basin\", \"shear zone\", \"fault\", \"batholith\", \"terrane\", \"inlier\",\n",
    "        \"province\", \"region\", \"mine\", \"group\", \"supergroup\", \"pluton\", \"complex\", \"gneiss\", \"channel\", \"graben\",\"domain\"\n",
    "    ]\n",
    "    s = s.lower().strip()\n",
    "    # Don't count plain country names as geounit\n",
    "    return any(kw in s for kw in geo_unit_keywords)\n",
    "\n",
    "def classify_location_type(loc):\n",
    "    loc = str(loc)\n",
    "    if is_gps(loc):\n",
    "        return \"GPS\"\n",
    "    elif is_approximate(loc):\n",
    "        return \"APPROXIMATE\"\n",
    "    elif is_geounit(loc):\n",
    "        return \"GEOLOGICAL_UNIT\"\n",
    "    elif len(loc) < 3:\n",
    "        return \"UNGEOCODED\"\n",
    "    else:\n",
    "        return \"PLACE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "112b0323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_base_place(location):\n",
    "    \"\"\"\n",
    "    Attempt to extract the most likely base location from a regional phrase.\n",
    "    - Strips out directional modifiers and keeps the likely toponyms.\n",
    "    - Works best for phrases like \"eastern part of X\", \"near Y\", etc.\n",
    "    \"\"\"\n",
    "    # 1. Try 'of ...' or 'in ...'\n",
    "    match = re.search(r'of ([A-Za-z \\-\\’\\'éèêàôîïç]+)$', location)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    match = re.search(r'in ([A-Za-z \\-\\’\\'éèêàôîïç]+)$', location)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    # 2. Remove phrases like 'part of', 'region of', 'border with'\n",
    "    cleaned = re.sub(r'^(north|south|east|west|northern|southern|eastern|western|central|upper|lower|border|area|part|region|near|around|about|along)\\s+of\\s+', '', location, flags=re.I)\n",
    "    cleaned = re.sub(r'^(north|south|east|west|northern|southern|eastern|western|central|upper|lower|border|area|part|region|near|around|about|along)\\s+', '', cleaned, flags=re.I)\n",
    "    cleaned = cleaned.strip(\",.;:()[] \")\n",
    "\n",
    "    # 3. If there are coordinates, skip\n",
    "    if re.search(r'\\d{4,}', cleaned):\n",
    "        return location.strip()\n",
    "\n",
    "    # 4. Return the last capitalized chunk (if any)\n",
    "    matches = re.findall(r'\\b([A-Z][a-zA-Z\\'\\-éèêàôîïç]+(?: [A-Z][a-zA-Z\\'\\-éèêàôîïç]+)*)\\b', cleaned)\n",
    "    if matches:\n",
    "        return matches[-1]\n",
    "\n",
    "    return cleaned.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8cccfdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent=\"geo_coding_example\")\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)  # Respect OSM's rate limits!\n",
    "\n",
    "# Geocode function with handling for failures\n",
    "def geopy_geocode(loc):\n",
    "    if not loc or loc.lower() in {\"gps\", \"na\", \"no-geocode-result\"}:\n",
    "        return pd.Series([\"no-geocode-result\"] * 4)\n",
    "    # Try full location first\n",
    "    try:\n",
    "        result = geocode(loc, language='en', addressdetails=True, timeout=10)\n",
    "        if result:\n",
    "            return pd.Series([result.latitude, result.longitude, result.address, \"APPROXIMATE\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Error geocoding '{loc}': {e}\")\n",
    "        # You can sleep here for rate limits if needed\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Try base place extraction as fallback\n",
    "    base = extract_base_place(loc)\n",
    "    if base and base.lower() != loc.lower():\n",
    "        try:\n",
    "            result = geocode(base, language='en', addressdetails=True, timeout=10)\n",
    "            if result:\n",
    "                return pd.Series([result.latitude, result.longitude, result.address, \"APPROXIMATE\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Error geocoding fallback '{base}': {e}\")\n",
    "            time.sleep(1)\n",
    "\n",
    "    return pd.Series([\"no-geocode-result\"] * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "449cdf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 2007_tshibubudze_themarkoyefault_2007_merged.csv\n",
      "Kept 107/108 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2007_tshibubudze_themarkoyefault_2007_merged_final.csv\n",
      "Processing: 2008_matabane_fe3_merged.csv\n",
      "Kept 146/146 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2008_matabane_fe3_merged_final.csv\n",
      "Processing: 2009_bontlenkuna_0605886p_honoursreport_merged.csv\n",
      "Kept 86/89 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2009_bontlenkuna_0605886p_honoursreport_merged_final.csv\n",
      "Processing: 2010_matsheka_irvinfinalthesis_merged.csv\n",
      "Kept 107/107 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2010_matsheka_irvinfinalthesis_merged_final.csv\n",
      "Processing: 2010_mohale_gisinterpretationofneburkinafaso_merged.csv\n",
      "Kept 92/93 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2010_mohale_gisinterpretationofneburkinafaso_merged_final.csv\n",
      "Processing: 2011_peters_eastmarkoye_2011_merged.csv\n",
      "Kept 151/152 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2011_peters_eastmarkoye_2011_merged_final.csv\n",
      "Processing: 2011_woolfe_thestratigraphyandmetamorphicfaciesofthekemb_merged.csv\n",
      "Kept 66/66 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2011_woolfe_thestratigraphyandmetamorphicfaciesofthekemb_merged_final.csv\n",
      "Processing: 2012_simoko_petrology,geochemistryandstructureofthepissilabatholithandthesaabazonegneiss_merged.csv\n",
      "Kept 53/54 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2012_simoko_petrology,geochemistryandstructureofthepissilabatholithandthesaabazonegneiss_merged_final.csv\n",
      "Processing: 2013_funyufunyu_merged.csv\n",
      "Kept 321/321 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2013_funyufunyu_merged_final.csv\n",
      "Processing: 2013_peters_merged.csv\n",
      "Kept 291/291 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2013_peters_merged_final.csv\n",
      "Processing: 2013_ramabulana_sadiolahillpetrology_merged.csv\n",
      "Kept 91/93 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2013_ramabulana_sadiolahillpetrology_merged_final.csv\n",
      "Processing: 2014_msc_yossi_merged.csv\n",
      "Kept 285/286 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2014_msc_yossi_merged_final.csv\n",
      "Processing: 2015_lebrun_siguiri_merged.csv\n",
      "Kept 524/526 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2015_lebrun_siguiri_merged_final.csv\n",
      "Processing: 2015_masurel_phd_merged.csv\n",
      "Kept 657/659 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2015_masurel_phd_merged_final.csv\n"
     ]
    }
   ],
   "source": [
    "def is_valid_location(loc):\n",
    "    \"\"\"Reject citations, years, and empty strings.\"\"\"\n",
    "    if not isinstance(loc, str) or not loc.strip():\n",
    "        return False\n",
    "    if \"et al\" in loc.lower():\n",
    "        return False\n",
    "    if re.search(r\"\\b(19|20)\\d{2}\\b\", loc):  # any year like 1998, 2006\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def filter_out_authors(df):\n",
    "    \"\"\"Filter rows that look like author names instead of real places.\"\"\"\n",
    "    denylist = {\"potrel\",\"björklund\",\"eriksson\",\"milési\",\"villeneuve\",\n",
    "                \"attoh\",\"ekwueme\",\"shanmugan\",\"sultan\"}\n",
    "    mask = df['location'].apply(\n",
    "        lambda x: is_valid_location(x) and x.lower() not in denylist\n",
    "    )\n",
    "    return df[mask].reset_index(drop=True)\n",
    "\n",
    "def clean_location_csv_folder(input_folder, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    for fname in os.listdir(input_folder):\n",
    "        if not fname.lower().endswith(\".csv\"):\n",
    "            continue\n",
    "        print(f\"Processing: {fname}\")\n",
    "        df = pd.read_csv(os.path.join(input_folder, fname))\n",
    "        original_len = len(df)\n",
    "\n",
    "        # Clean location column\n",
    "        if 'location' in df.columns:\n",
    "            df['location'] = df['location'].map(remove_generic_location_words)\n",
    "        else:\n",
    "            print(f\"Skipping {fname}: no 'location' column\")\n",
    "            continue\n",
    "\n",
    "        # Drop empty after cleaning\n",
    "        df_cleaned = df[df['location'].astype(str).str.strip().astype(bool)].reset_index(drop=True)\n",
    "\n",
    "        # Remove authors (denylist + regex)\n",
    "        if {'location', 'mentions'}.issubset(df_cleaned.columns):\n",
    "            df_cleaned = filter_out_authors(df_cleaned)\n",
    "\n",
    "        # Classify location type\n",
    "        if 'location' in df_cleaned.columns:\n",
    "            df_cleaned['location_type'] = df_cleaned['location'].map(classify_location_type)\n",
    "        else:\n",
    "            df_cleaned['location_type'] = 'UNGEOCODED'\n",
    "\n",
    "        print(f\"Kept {len(df_cleaned)}/{original_len} locations after filtering.\")\n",
    "        # Save\n",
    "        outpath = os.path.join(output_folder, fname.replace(\".csv\", \"_final.csv\"))\n",
    "        df_cleaned.to_csv(outpath, index=False)\n",
    "        print(f\"Saved cleaned: {outpath}\")\n",
    "\n",
    "clean_location_csv_folder(\"DATA_FOR_MODELS/Data4_merged2\", \"DATA_FOR_MODELS/Data_merged_clean\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2f8701e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geocoding: 2007_tshibubudze_themarkoyefault_2007_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2007_tshibubudze_themarkoyefault_2007_merged_final_geocoded.csv\n",
      "Geocoding: 2008_matabane_fe3_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2008_matabane_fe3_merged_final_geocoded.csv\n",
      "Geocoding: 2009_bontlenkuna_0605886p_honoursreport_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2009_bontlenkuna_0605886p_honoursreport_merged_final_geocoded.csv\n",
      "Geocoding: 2010_matsheka_irvinfinalthesis_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2010_matsheka_irvinfinalthesis_merged_final_geocoded.csv\n",
      "Geocoding: 2010_mohale_gisinterpretationofneburkinafaso_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2010_mohale_gisinterpretationofneburkinafaso_merged_final_geocoded.csv\n",
      "Geocoding: 2011_peters_eastmarkoye_2011_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2011_peters_eastmarkoye_2011_merged_final_geocoded.csv\n",
      "Geocoding: 2011_woolfe_thestratigraphyandmetamorphicfaciesofthekemb_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2011_woolfe_thestratigraphyandmetamorphicfaciesofthekemb_merged_final_geocoded.csv\n",
      "Geocoding: 2012_simoko_petrology,geochemistryandstructureofthepissilabatholithandthesaabazonegneiss_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2012_simoko_petrology,geochemistryandstructureofthepissilabatholithandthesaabazonegneiss_merged_final_geocoded.csv\n",
      "Geocoding: 2013_funyufunyu_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2013_funyufunyu_merged_final_geocoded.csv\n",
      "Geocoding: 2013_peters_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2013_peters_merged_final_geocoded.csv\n",
      "Geocoding: 2013_ramabulana_sadiolahillpetrology_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2013_ramabulana_sadiolahillpetrology_merged_final_geocoded.csv\n",
      "Geocoding: 2014_msc_yossi_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2014_msc_yossi_merged_final_geocoded.csv\n",
      "Geocoding: 2015_lebrun_siguiri_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2015_lebrun_siguiri_merged_final_geocoded.csv\n",
      "Geocoding: 2015_masurel_phd_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2015_masurel_phd_merged_final_geocoded.csv\n"
     ]
    }
   ],
   "source": [
    "# Patterns \n",
    "_DEC_PAIR = re.compile(r'^\\s*([+-]?\\d+(?:\\.\\d+)?)\\s*[,;\\s]\\s*([+-]?\\d+(?:\\.\\d+)?)\\s*$')\n",
    "_UTM_WITH_ZONE = re.compile(r'^\\s*(\\d{1,2}|60)\\s*([NSns])\\s*[,;\\s]\\s*(\\d{5,7})\\s*[,;\\s]\\s*(\\d{6,8})\\s*$')\n",
    "# NEW: UTM zone + band (e.g., \"30P 694686; 1443104\")\n",
    "_UTM_WITH_BAND = re.compile(r'^\\s*(\\d{1,2}|60)\\s*([C-HJ-NP-Xc-hj-np-x])\\s*[,;\\s]\\s*(\\d{5,7})\\s*[,;\\s]\\s*(\\d{6,8})\\s*$')\n",
    "# Keep your \"bare\" — also matches dash \"0184836-1587581\"\n",
    "_UTM_BARE = re.compile(r'^\\s*(?:GPS\\s*=\\s*)?(\\d{6,8})\\s*[-;,]\\s*(\\d{6,8})\\s*$')\n",
    "# NEW: \"0805000 E, 1594000 N\" or \"570950E and 1245150N\"\n",
    "_UTM_E_AND_N = re.compile(r'(?ix)\\b(?P<e>\\d{5,7})\\s*(?:m?E|E)\\b.*?(?P<n>\\d{6,8})\\s*(?:m?N|N)\\b')\n",
    "# NEW: \"WGS84, UTM 29N, East.: 415178, North.: 1290080\"\n",
    "_UTM_LABELLED = re.compile(\n",
    "    r'(?ix)\\b(?:WGS84\\s*,\\s*)?UTM\\s*(?P<zone>\\d{1,2})(?P<tail>[A-Z])?'\n",
    "    r'\\s*,?\\s*East\\.?:\\s*(?P<e>\\d{5,7})\\s*,\\s*North\\.?:\\s*(?P<n>\\d{6,8})'\n",
    ")\n",
    "\n",
    "def _dms_to_dec(hem, d, m, s_whole, s_tenths):\n",
    "    d = int(d); m = int(m); s = float(f\"{int(s_whole)}.{int(s_tenths)}\")\n",
    "    if s >= 60: m += int(s // 60); s %= 60\n",
    "    if m >= 60: d += int(m // 60); m %= 60\n",
    "    dec = d + m/60.0 + s/3600.0\n",
    "    return -dec if hem and hem.upper() in (\"S\",\"W\") else dec\n",
    "\n",
    "# working split DMS: \"N 14 36 37 8 E 00 00 12 1\"\n",
    "def parse_split_dms(txt: str):\n",
    "    p = str(txt).strip().split()\n",
    "    # Allow the lat hemisphere to be missing (assume N) and accept \"GPS\" token at start\n",
    "    tokens = [t for t in p if t.upper() != \"GPS\"]\n",
    "    if len(tokens) == 10 and tokens[0] in (\"N\",\"S\") and tokens[5] in (\"E\",\"W\"):\n",
    "        Hlat, d1,m1,s1,st1, Hlon, d2,m2,s2,st2 = tokens\n",
    "    elif len(tokens) == 9 and tokens[5] in (\"E\",\"W\"):\n",
    "        # e.g., \"14 36 37 8 E 00 00 12 1\" → assume N for latitude\n",
    "        Hlat, d1,m1,s1,st1 = \"N\", tokens[0],tokens[1],tokens[2],tokens[3]\n",
    "        Hlon, d2,m2,s2,st2 = tokens[4],tokens[5],tokens[6],tokens[7],tokens[8]\n",
    "    else:\n",
    "        return (np.nan, np.nan)\n",
    "    try:\n",
    "        lat = _dms_to_dec(Hlat, d1, m1, s1, st1)\n",
    "        lon = _dms_to_dec(Hlon, d2, m2, s2, st2)\n",
    "        return (lat, lon) if (-90 <= lat <= 90 and -180 <= lon <= 180) else (np.nan, np.nan)\n",
    "    except:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "def _band_to_hem(band: str) -> str:\n",
    "    b = band.upper()\n",
    "    # UTM latitude bands\n",
    "    return \"S\" if \"C\" <= b <= \"M\" else \"N\"\n",
    "\n",
    "def geocode_location_csv_folder(input_folder, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    for fname in os.listdir(input_folder):\n",
    "        if not fname.lower().endswith(\".csv\"):\n",
    "            continue\n",
    "        print(f\"Geocoding: {fname}\")\n",
    "        df = pd.read_csv(os.path.join(input_folder, fname))\n",
    "        if 'location' not in df.columns:\n",
    "            print(f\"SKIP (no location column): {fname}\")\n",
    "            continue\n",
    "\n",
    "        # Keep LLM & LLM,Rules (skip rules-only)\n",
    "        df = df[df['source'].astype(str).str.lower().str.contains('llm')].copy()\n",
    "\n",
    "        #PARSE ON RAW TEXT\n",
    "        raw_loc = df['location'].astype(str).str.strip()\n",
    "\n",
    "        # decimal\n",
    "        dec = raw_loc.str.extract(_DEC_PAIR)\n",
    "        dec_lat = pd.to_numeric(dec[0], errors='coerce')\n",
    "        dec_lon = pd.to_numeric(dec[1], errors='coerce')\n",
    "        good_dec = dec_lat.between(-90,90) & dec_lon.between(-180,180)\n",
    "\n",
    "        # UTM with zone+hemisphere (e.g., \"30 N, 694686, 1443104\")\n",
    "        utm = raw_loc.str.extract(_UTM_WITH_ZONE)\n",
    "        utm_zone = pd.to_numeric(utm[0], errors='coerce')\n",
    "        utm_hem  = utm[1].str.upper()\n",
    "        utm_e    = pd.to_numeric(utm[2], errors='coerce')\n",
    "        utm_n    = pd.to_numeric(utm[3], errors='coerce')\n",
    "        mask_utm = utm_zone.notna() & utm_hem.notna() & utm_e.notna() & utm_n.notna()\n",
    "\n",
    "        utm_lat = pd.Series(np.nan, index=df.index, dtype=float)\n",
    "        utm_lon = pd.Series(np.nan, index=df.index, dtype=float)\n",
    "        if mask_utm.any():\n",
    "            uniq = pd.DataFrame({'z': utm_zone, 'h': utm_hem}).dropna().drop_duplicates()\n",
    "            transformers = {(int(z), h): Transformer.from_crs(\n",
    "                f\"EPSG:{'326' if h=='N' else '327'}{int(z):02d}\", \"EPSG:4326\", always_xy=True)\n",
    "                for z, h in uniq.itertuples(index=False)}\n",
    "            for (z,h), T in transformers.items():\n",
    "                rows = (utm_zone==z) & (utm_hem==h) & mask_utm\n",
    "                if rows.any():\n",
    "                    lon_vals, lat_vals = T.transform(utm_e[rows].astype(float).values,\n",
    "                                                     utm_n[rows].astype(float).values)\n",
    "                    utm_lon.loc[rows] = lon_vals\n",
    "                    utm_lat.loc[rows] = lat_vals\n",
    "        good_utm = utm_lat.notna() & utm_lon.notna()\n",
    "\n",
    "        # UTM with zone+band (e.g., \"30P 694686; 1443104\")\n",
    "        utmb = raw_loc.str.extract(_UTM_WITH_BAND)\n",
    "        if isinstance(utmb, pd.DataFrame) and not utmb.empty:\n",
    "            utmb_zone = pd.to_numeric(utmb[0], errors='coerce')\n",
    "            utmb_band = utmb[1].str.upper()\n",
    "            utmb_e    = pd.to_numeric(utmb[2], errors='coerce')\n",
    "            utmb_n    = pd.to_numeric(utmb[3], errors='coerce')\n",
    "            mask_utmb = utmb_zone.notna() & utmb_band.notna() & utmb_e.notna() & utmb_n.notna()\n",
    "            if mask_utmb.any():\n",
    "                # Infer hemisphere from band letter\n",
    "                utmb_hem = utmb_band.map(_band_to_hem)\n",
    "                uniqb = pd.DataFrame({'z': utmb_zone, 'h': utmb_hem}).dropna().drop_duplicates()\n",
    "                transformers_b = {(int(z), h): Transformer.from_crs(\n",
    "                    f\"EPSG:{'326' if h=='N' else '327'}{int(z):02d}\", \"EPSG:4326\", always_xy=True)\n",
    "                    for z, h in uniqb.itertuples(index=False)}\n",
    "                utmb_lat = pd.Series(np.nan, index=df.index, dtype=float)\n",
    "                utmb_lon = pd.Series(np.nan, index=df.index, dtype=float)\n",
    "                for (z,h), T in transformers_b.items():\n",
    "                    rows = (utmb_zone==z) & (utmb_hem==h) & mask_utmb\n",
    "                    if rows.any():\n",
    "                        lon_vals, lat_vals = T.transform(utmb_e[rows].astype(float).values,\n",
    "                                                         utmb_n[rows].astype(float).values)\n",
    "                        utmb_lon.loc[rows] = lon_vals\n",
    "                        utmb_lat.loc[rows] = lat_vals\n",
    "                # Merge these into UTM results (prefer explicit zone+band over heuristics)\n",
    "                overwrite = utmb_lat.notna() & utmb_lon.notna()\n",
    "                utm_lat.loc[overwrite] = utmb_lat[overwrite]\n",
    "                utm_lon.loc[overwrite] = utmb_lon[overwrite]\n",
    "                good_utm = utm_lat.notna() & utm_lon.notna()\n",
    "\n",
    "        # split-DMS (\"N 14 36 37 8 E 00 00 12 1\" and also allow missing N)\n",
    "        dms_latlon = raw_loc.apply(lambda s: pd.Series(parse_split_dms(s), index=['lat_dms','lon_dms']))\n",
    "        dms_lat, dms_lon = dms_latlon['lat_dms'], dms_latlon['lon_dms']\n",
    "        good_dms = dms_lat.notna() & dms_lon.notna()\n",
    "\n",
    "        # UTM labelled \"E/N\" (e.g., \"0805000 E, 1594000 N\" / \"570950E and 1245150N\")\n",
    "        en = raw_loc.str.extract(_UTM_E_AND_N)\n",
    "        en_e = pd.to_numeric(en['e'], errors='coerce')\n",
    "        en_n = pd.to_numeric(en['n'], errors='coerce')\n",
    "        good_en = en_e.notna() & en_n.notna()\n",
    "\n",
    "        # bare UTM like \"1534765; 0216396\" or \"0184836-1587581\"\n",
    "        bare = raw_loc.str.extract(_UTM_BARE)\n",
    "        a = pd.to_numeric(bare[0], errors='coerce'); b = pd.to_numeric(bare[1], errors='coerce')\n",
    "        e_guess, n_guess = a.copy(), b.copy()\n",
    "        swap = a > b  # sometimes northing first\n",
    "        e_guess[swap], n_guess[swap] = b[swap], a[swap]\n",
    "        good_bare_pair = e_guess.between(100000,900000) & n_guess.between(0,10000000)\n",
    "\n",
    "        # Heuristic convert EN (no zone): try zones 30/31/32; keep those within Burkina Faso bbox\n",
    "        def _heuristic_utm_to_ll(e_ser, n_ser, cond):\n",
    "            out_lat = pd.Series(np.nan, index=df.index, dtype=float)\n",
    "            out_lon = pd.Series(np.nan, index=df.index, dtype=float)\n",
    "            if not cond.any(): return out_lat, out_lon\n",
    "            for z in (30,31,32):\n",
    "                T = Transformer.from_crs(f\"EPSG:326{z:02d}\", \"EPSG:4326\", always_xy=True)\n",
    "                rows = cond & out_lat.isna()\n",
    "                if rows.any():\n",
    "                    lon_vals, lat_vals = T.transform(e_ser[rows].astype(float).values,\n",
    "                                                     n_ser[rows].astype(float).values)\n",
    "                    inside = (lat_vals >= 9) & (lat_vals <= 16) & (lon_vals >= -6) & (lon_vals <= 3)\n",
    "                    if inside.any():\n",
    "                        idx = e_ser[rows].index[inside]\n",
    "                        out_lon.loc[idx] = np.array(lon_vals)[inside]\n",
    "                        out_lat.loc[idx] = np.array(lat_vals)[inside]\n",
    "            return out_lat, out_lon\n",
    "\n",
    "        en_lat, en_lon = _heuristic_utm_to_ll(en_e, en_n, good_en)\n",
    "        bare_lat, bare_lon = _heuristic_utm_to_ll(e_guess, n_guess, good_bare_pair)\n",
    "\n",
    "        good_en_ll = en_lat.notna() & en_lon.notna()\n",
    "        good_bare = bare_lat.notna() & bare_lon.notna()\n",
    "\n",
    "        # NOW clean and geocode your normal way\n",
    "        df['location'] = df['location'].map(remove_generic_location_words)\n",
    "        geo_df = df['location'].apply(lambda loc: pd.Series(geopy_geocode(loc)))\n",
    "        geo_df.columns = ['geocode_lat', 'geocode_lon', 'geocode_str', 'geocode_type']\n",
    "        df = pd.concat([df, geo_df], axis=1)\n",
    "\n",
    "        # Ensure we can overwrite \"no-geocode-result\"\n",
    "        df['geocode_lat'] = pd.to_numeric(df['geocode_lat'], errors='coerce')\n",
    "        df['geocode_lon'] = pd.to_numeric(df['geocode_lon'], errors='coerce')\n",
    "        if 'location_type' not in df.columns:\n",
    "            df['location_type'] = np.nan\n",
    "\n",
    "        # OVERRIDE (priority: bare/en < DMS < UTM < decimal), and stamp as GPS exact\n",
    "        # 1) bare UTM heuristic\n",
    "        df.loc[good_bare, 'geocode_lat']  = bare_lat[good_bare].astype(float)\n",
    "        df.loc[good_bare, 'geocode_lon']  = bare_lon[good_bare].astype(float)\n",
    "        df.loc[good_bare, 'geocode_type'] = 'utm_heuristic'\n",
    "        df.loc[good_bare, 'geocode_str']  = \"GPS: \" + bare_lat[good_bare].round(6).astype(str) + \", \" + bare_lon[good_bare].round(6).astype(str)\n",
    "        df.loc[good_bare, 'accuracy']     = 'EXTRACTED'\n",
    "        df.loc[good_bare, 'location_type'] = 'GPS'\n",
    "\n",
    "        # 1b) E/N labelled heuristic\n",
    "        df.loc[good_en_ll, 'geocode_lat']  = en_lat[good_en_ll].astype(float)\n",
    "        df.loc[good_en_ll, 'geocode_lon']  = en_lon[good_en_ll].astype(float)\n",
    "        df.loc[good_en_ll, 'geocode_type'] = 'utm_heuristic'\n",
    "        df.loc[good_en_ll, 'geocode_str']  = \"GPS: \" + en_lat[good_en_ll].round(6).astype(str) + \", \" + en_lon[good_en_ll].round(6).astype(str)\n",
    "        df.loc[good_en_ll, 'accuracy']     = 'EXTRACTED'\n",
    "        df.loc[good_en_ll, 'location_type'] = 'GPS'\n",
    "\n",
    "        # 2) DMS split\n",
    "        df.loc[good_dms, 'geocode_lat']  = dms_lat[good_dms].astype(float)\n",
    "        df.loc[good_dms, 'geocode_lon']  = dms_lon[good_dms].astype(float)\n",
    "        df.loc[good_dms, 'geocode_type'] = 'dms'\n",
    "        df.loc[good_dms, 'geocode_str']  = \"GPS: \" + dms_lat[good_dms].round(6).astype(str) + \", \" + dms_lon[good_dms].round(6).astype(str)\n",
    "        df.loc[good_dms, 'accuracy']     = 'EXTRACTED'\n",
    "        df.loc[good_dms, 'location_type'] = 'GPS'\n",
    "\n",
    "        # 3) UTM (zone+hem or zone+band, incl. \"UTM 29N East./North.\")\n",
    "        df.loc[good_utm, 'geocode_lat']  = utm_lat[good_utm].astype(float)\n",
    "        df.loc[good_utm, 'geocode_lon']  = utm_lon[good_utm].astype(float)\n",
    "        df.loc[good_utm, 'geocode_type'] = 'utm'\n",
    "        df.loc[good_utm, 'geocode_str']  = \"GPS: \" + utm_lat[good_utm].round(6).astype(str) + \", \" + utm_lon[good_utm].round(6).astype(str)\n",
    "        df.loc[good_utm, 'accuracy']     = 'EXTRACTED'\n",
    "        df.loc[good_utm, 'location_type'] = 'GPS'\n",
    "\n",
    "        # 4) decimal\n",
    "        df.loc[good_dec, 'geocode_lat']  = dec_lat[good_dec].astype(float)\n",
    "        df.loc[good_dec, 'geocode_lon']  = dec_lon[good_dec].astype(float)\n",
    "        df.loc[good_dec, 'geocode_type'] = 'decimal'\n",
    "        df.loc[good_dec, 'geocode_str']  = \"GPS: \" + dec_lat[good_dec].round(6).astype(str) + \", \" + dec_lon[good_dec].round(6).astype(str)\n",
    "        df.loc[good_dec, 'accuracy']     = 'EXTRACTED'\n",
    "        df.loc[good_dec, 'location_type'] = 'GPS'\n",
    "\n",
    "        out_csv = os.path.join(output_folder, fname.replace(\".csv\", \"_geocoded.csv\"))\n",
    "        df.to_csv(out_csv, index=False)\n",
    "        print(f\"Saved geocoded file: {out_csv}\")\n",
    "\n",
    "# Run\n",
    "geocode_location_csv_folder(\"DATA_FOR_MODELS/Data_merged_clean\", \"DATA_FOR_MODELS/Data_geocoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f6dae2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2007_tshibubudze_themarkoyefault_2007_merged_final_geocoded_map.html\n",
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2008_matabane_fe3_merged_final_geocoded_map.html\n",
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2009_bontlenkuna_0605886p_honoursreport_merged_final_geocoded_map.html\n",
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2010_matsheka_irvinfinalthesis_merged_final_geocoded_map.html\n",
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2010_mohale_gisinterpretationofneburkinafaso_merged_final_geocoded_map.html\n",
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2011_peters_eastmarkoye_2011_merged_final_geocoded_map.html\n",
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2011_woolfe_thestratigraphyandmetamorphicfaciesofthekemb_merged_final_geocoded_map.html\n",
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2012_simoko_petrology,geochemistryandstructureofthepissilabatholithandthesaabazonegneiss_merged_final_geocoded_map.html\n",
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2013_funyufunyu_merged_final_geocoded_map.html\n",
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2013_peters_merged_final_geocoded_map.html\n",
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2013_ramabulana_sadiolahillpetrology_merged_final_geocoded_map.html\n",
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2014_msc_yossi_merged_final_geocoded_map.html\n",
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2015_lebrun_siguiri_merged_final_geocoded_map.html\n",
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2015_masurel_phd_merged_final_geocoded_map.html\n"
     ]
    }
   ],
   "source": [
    "# --- Paths ---\n",
    "input_dir = Path(\"DATA_FOR_MODELS/Data_geocoded\")\n",
    "output_dir = Path(\"DATA_FOR_MODELS/Data_Maps\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Function to build a map for one file ---\n",
    "def build_map(csv_path, output_dir):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # keep only valid coords\n",
    "    df = df[df['geocode_lat'].apply(lambda x: str(x).replace('.', '', 1).isdigit())]\n",
    "    df['geocode_lat'] = df['geocode_lat'].astype(float)\n",
    "    df['geocode_lon'] = df['geocode_lon'].astype(float)\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\" Skipping {csv_path.name} (no valid coordinates)\")\n",
    "        return\n",
    "\n",
    "    # --- accuracy: exact or approximate ---\n",
    "    def accuracy(row):\n",
    "        gstr = str(row.get(\"geocode_str\", \"\")).lower()\n",
    "        if \"approx\" in gstr or \"near\" in gstr:\n",
    "            return \"approximate\"\n",
    "        return \"exact\"\n",
    "\n",
    "    df[\"accuracy\"] = df.apply(accuracy, axis=1)\n",
    "\n",
    "    # --- assign colors automatically per unique location_type ---\n",
    "    unique_types = df[\"location_type\"].dropna().unique()\n",
    "    color_cycle = itertools.cycle(\n",
    "        [\"blue\",\"green\",\"red\",\"purple\",\"orange\",\"cyan\",\"pink\",\"brown\",\"gray\",\"olive\"]\n",
    "    )\n",
    "    COLOR_MAP = {lt: next(color_cycle) for lt in unique_types}\n",
    "\n",
    "    def color_for(row):\n",
    "        return COLOR_MAP.get(row.get(\"location_type\", \"\"), \"black\")\n",
    "\n",
    "    # --- center map ---\n",
    "    center_lat = df[\"geocode_lat\"].mean()\n",
    "    center_lon = df[\"geocode_lon\"].mean()\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=6)\n",
    "\n",
    "    # --- plot circles ---\n",
    "    for _, row in df.iterrows():\n",
    "        popup_text = (\n",
    "            f\"<b>Location:</b> {row.get('location_clean', '')}<br>\"\n",
    "            f\"<b>Context:</b> {row.get('mentions', row.get('mention', ''))}<br>\"\n",
    "            f\"<b>Geocode:</b> {row.get('geocode_str', '')}<br>\"\n",
    "            f\"<b>Pages:</b> {row.get('pages', '')}<br>\"\n",
    "            f\"<b>Type:</b> {row.get('location_type', '')}<br>\"\n",
    "            f\"<b>Accuracy:</b> {row['accuracy']}\"\n",
    "        )\n",
    "\n",
    "        folium.CircleMarker(\n",
    "            location=[row[\"geocode_lat\"], row[\"geocode_lon\"]],\n",
    "            radius=7,\n",
    "            color=color_for(row),\n",
    "            fill=True,\n",
    "            fill_color=color_for(row),\n",
    "            fill_opacity=0.7 if row[\"accuracy\"] == \"exact\" else 0.3,\n",
    "            tooltip=row.get(\"location_clean\", \"\"),\n",
    "            popup=folium.Popup(popup_text, max_width=350),\n",
    "        ).add_to(m)\n",
    "\n",
    "    # --- build legend dynamically ---\n",
    "    legend_html = \"\"\"\n",
    "    <div style=\"position: fixed; bottom: 20px; left: 20px; z-index: 9999;\n",
    "                background: white; padding: 8px 10px; border: 1px solid #ccc; border-radius: 4px;\">\n",
    "      <div style=\"font-weight: 600; margin-bottom: 6px;\">Legend (location_type)</div>\n",
    "    \"\"\"\n",
    "    for lt, col in COLOR_MAP.items():\n",
    "        legend_html += f\"\"\"\n",
    "          <div><span style=\"display:inline-block;width:12px;height:12px;background:{col};\n",
    "                             margin-right:6px;border-radius:50%;\"></span>{lt}</div>\n",
    "        \"\"\"\n",
    "    legend_html += \"\"\"\n",
    "      <div style=\"margin-top:6px;\"><i>Transparency: solid = exact, faint = approximate</i></div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    m.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "    # --- save map ---\n",
    "    out_path = output_dir / (csv_path.stem + \"_map.html\")\n",
    "    m.save(out_path)\n",
    "    print(f\"Saved map: {out_path}\")\n",
    "\n",
    "# --- Process all CSV files ---\n",
    "for csv_file in input_dir.glob(\"*.csv\"):\n",
    "    build_map(csv_file, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
