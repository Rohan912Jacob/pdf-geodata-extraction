{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aN_ttOj3bqD1",
    "outputId": "c98c4235-03ff-43a8-bc1b-6cf663530198"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopy in /usr/local/lib/python3.12/dist-packages (2.4.1)\n",
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.12/dist-packages (from geopy) (2.1)\n",
      "Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: unidecode\n",
      "Successfully installed unidecode-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install geopy unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hUD_PdCaC4Fh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zauIIFX0o1MR"
   },
   "source": [
    "Merging the LLM locations and rules locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GrIYsaJ2HBK6"
   },
   "outputs": [],
   "source": [
    "def agg_pages(series):\n",
    "    return ','.join(sorted({str(x) for x in series if pd.notna(x)}))\n",
    "\n",
    "def agg_mentions(series):\n",
    "    return \" || \".join(sorted({str(x).strip() for x in series if pd.notna(x)}))\n",
    "\n",
    "def get_base_key(fname):\n",
    "    # for match\n",
    "    return (fname.lower()\n",
    "            .replace('_locations.csv', '')\n",
    "            .replace('_locations_llm_clean.csv', '')\n",
    "            .replace('.csv', '')\n",
    "            .replace(' ', '')\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WhyF6J2bG1sg"
   },
   "outputs": [],
   "source": [
    "def merge_rule_llm_pair(rules_csv, llm_csv, out_csv):\n",
    "    import pandas as pd\n",
    "\n",
    "    # RULES\n",
    "    df_rules = pd.read_csv(rules_csv)\n",
    "    if not {'filename', 'page', 'mention', 'location'}.issubset(df_rules.columns):\n",
    "        print(f\"Skipping {rules_csv}: missing required columns\")\n",
    "        return\n",
    "    # Label as rules\n",
    "    df_rules = df_rules[['filename', 'page', 'mention', 'location']].copy()\n",
    "    df_rules[\"source\"] = \"rules\"\n",
    "\n",
    "    # LLM\n",
    "    df_llm = pd.read_csv(llm_csv)\n",
    "    if not {'filename', 'page', 'mention', 'location'}.issubset(df_llm.columns):\n",
    "        print(f\"Skipping {llm_csv}: missing required columns\")\n",
    "        return\n",
    "    df_llm = df_llm[['filename', 'page', 'mention', 'location']].copy()\n",
    "    df_llm[\"source\"] = \"llm\"\n",
    "\n",
    "    # Combine\n",
    "    df_all = pd.concat([df_rules, df_llm], ignore_index=True)\n",
    "\n",
    "    # Group and Merge Sources\n",
    "    df_merged = (\n",
    "        df_all\n",
    "        .groupby(['filename', 'location'])\n",
    "        .agg({\n",
    "            'page': agg_pages,\n",
    "            'mention': agg_mentions,\n",
    "            'source': lambda s: ','.join(sorted(set(s))),\n",
    "        })\n",
    "        .reset_index()\n",
    "        .rename(columns={'page': 'pages', 'mention': 'mentions'})\n",
    "    )\n",
    "\n",
    "\n",
    "    df_merged.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved merged file: {out_csv}\")\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "c6VS7FYIHGr1"
   },
   "outputs": [],
   "source": [
    "def merge_all_rule_llm_files(rules_folder, llm_folder, out_folder):\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    rules_files = {get_base_key(f): f for f in os.listdir(rules_folder) if f.endswith(\"_locations.csv\")}\n",
    "    llm_files = {get_base_key(f): f for f in os.listdir(llm_folder) if \"_llm_clean.csv\" in f.lower()}\n",
    "    matched = rules_files.keys() & llm_files.keys()\n",
    "    print(f\"RULES base keys: {list(rules_files.keys())}\")\n",
    "    print(f\"LLM base keys: {list(llm_files.keys())}\")\n",
    "    print(f\"Found {len(matched)} matching base keys to merge.\")\n",
    "    for key in matched:\n",
    "        rules_path = os.path.join(rules_folder, rules_files[key])\n",
    "        llm_path = os.path.join(llm_folder, llm_files[key])\n",
    "        out_path = os.path.join(out_folder, f\"{key}_merged.csv\")\n",
    "        merge_rule_llm_pair(rules_path, llm_path, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tqqzTlJ6HL3P",
    "outputId": "8da8e0c6-5568-4cd1-b307-b27d80502269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RULES base keys: ['2007_tshibubudze_themarkoyefault_2007', '2008_matabane_fe3', '2009_bontlenkuna_0605886p_honoursreport', '2010_matsheka_irvinfinalthesis', '2010_mohale_gisinterpretationofneburkinafaso', '2011_peters_eastmarkoye_2011', '2011_woolfe_thestratigraphyandmetamorphicfaciesofthekemb', '2012_simoko_petrology,geochemistryandstructureofthepissilabatholithandthesaabazonegneiss', '2013_funyufunyu', '2013_peters', '2013_ramabulana_sadiolahillpetrology', '2014_msc_yossi', '2015_lebrun_siguiri', '2015_masurel_phd']\n",
      "LLM base keys: ['2007_tshibubudze_themarkoyefault_2007', '2008_matabane_fe3', '2009_bontlenkuna_0605886p_honoursreport', '2010_matsheka_irvinfinalthesis', '2010_mohale_gisinterpretationofneburkinafaso', '2011_peters_eastmarkoye_2011', '2011_woolfe_thestratigraphyandmetamorphicfaciesofthekemb', '2012_simoko_petrology,geochemistryandstructureofthepissilabatholithandthesaabazonegneiss', '2013_funyufunyu', '2013_peters', '2013_ramabulana_sadiolahillpetrology', '2014_msc_yossi', '2015_lebrun_siguiri', '2015_masurel_phd']\n",
      "Found 14 matching base keys to merge.\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2015_masurel_phd_merged.csv\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2007_tshibubudze_themarkoyefault_2007_merged.csv\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2012_simoko_petrology,geochemistryandstructureofthepissilabatholithandthesaabazonegneiss_merged.csv\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2011_woolfe_thestratigraphyandmetamorphicfaciesofthekemb_merged.csv\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2013_ramabulana_sadiolahillpetrology_merged.csv\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2011_peters_eastmarkoye_2011_merged.csv\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2008_matabane_fe3_merged.csv\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2010_mohale_gisinterpretationofneburkinafaso_merged.csv\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2013_peters_merged.csv\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2009_bontlenkuna_0605886p_honoursreport_merged.csv\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2014_msc_yossi_merged.csv\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2015_lebrun_siguiri_merged.csv\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2010_matsheka_irvinfinalthesis_merged.csv\n",
      "Saved merged file: DATA_FOR_MODELS/Data4_merged2\\2013_funyufunyu_merged.csv\n"
     ]
    }
   ],
   "source": [
    "merge_all_rule_llm_files(\"DATA_FOR_MODELS/Data4_rules_csv\", \"DATA_FOR_MODELS/Data_LLM_CSV_clean\", \"DATA_FOR_MODELS/Data4_merged2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSq7IQISo9SA"
   },
   "source": [
    "Cleaning the merged files:\n",
    "\n",
    "Removing authors due to in text citation\n",
    "\n",
    "Removing generic location names e.g. study area, basin\n",
    "\n",
    "Categorizing location by type: GPS,PLACE and Approximate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "EG0S827_M5-v"
   },
   "outputs": [],
   "source": [
    "def is_citation(mention, loc):\n",
    "    # Looks for patterns like \"Ledru et al.\", \"(Taylor, 1990)\", etc.\n",
    "    return bool(re.search(rf\"\\b{re.escape(loc)}\\b.*et al\\.?\", mention, re.IGNORECASE)) or \\\n",
    "           bool(re.search(rf\"\\({re.escape(loc)}, \\d{{4}}\\)\", mention))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "KdCvml6VNBIg"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def is_person_entity(loc):\n",
    "    doc = nlp(loc)\n",
    "    return any(ent.label_ == \"PERSON\" for ent in doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "YIhmQAsDNF7D"
   },
   "outputs": [],
   "source": [
    "def is_author_like(loc, mention):\n",
    "    loc_clean = loc.strip()\n",
    "    if is_citation(mention, loc_clean):\n",
    "        return True\n",
    "    if is_person_entity(loc_clean):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "TibwCLWQNNow"
   },
   "outputs": [],
   "source": [
    "def filter_out_authors(df):\n",
    "    # Expects 'location' and 'mentions' columns\n",
    "    mask = df.apply(lambda row: not is_author_like(row['location'], row['mentions']), axis=1)\n",
    "    return df[mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "xT0D088BizUX"
   },
   "outputs": [],
   "source": [
    "GENERIC_LOCATION_WORDS = [\n",
    "    \"study area\", \"area of study\", \"area of investigation\", \"study region\", \"study site\",\n",
    "    \"study location\", \"area\", \"region\", \"zone\", \"locality\", \"localities\",\n",
    "    \"the area\", \"the region\", \"investigation area\", \"site\", \"sites\"\n",
    "]\n",
    "\n",
    "def remove_generic_location_words(location):\n",
    "    \"\"\"\n",
    "    Removes generic location words/phrases (case-insensitive) from start or end of a location string.\n",
    "    \"\"\"\n",
    "    if not isinstance(location, str):\n",
    "        return location\n",
    "    s = location.strip()\n",
    "    # Build a regex to match generic words at start or end (with optional spaces/punctuation)\n",
    "    pattern = r\"^(%s)\\b[\\s,:\\-]*|[\\s,:\\-]*(%s)$\" % (\n",
    "        \"|\".join(map(re.escape, GENERIC_LOCATION_WORDS)),\n",
    "        \"|\".join(map(re.escape, GENERIC_LOCATION_WORDS))\n",
    "    )\n",
    "    s = re.sub(pattern, \"\", s, flags=re.IGNORECASE)\n",
    "    # Remove double spaces, commas, etc.\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = s.strip(\",.:-; \")\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "LB0PfeR_VJTc"
   },
   "outputs": [],
   "source": [
    "def is_gps(s):\n",
    "    # Simple patterns for decimal/sexagesimal degrees or UTM\n",
    "    s = str(s).strip()\n",
    "    # Simple decimal degree/UTM/Easting-Northing patterns:\n",
    "    if re.match(r\"^\\d{5,}[-; ]\\s*\\d{5,}$\", s):  # e.g., 0184836-1587581 or 1534836; 0216256\n",
    "        return True\n",
    "    # Matches \"N 14 36 37 8 E 00 00 12 1\" etc.\n",
    "    if re.search(r\"[NSEW]\\s*\\d+\", s) and re.search(r\"[EW]\\s*\\d+\", s):\n",
    "        return True\n",
    "    # Matches \"GPS\" in string (sometimes extracted as \"GPS 14 35 05 7 W 00 00 05 4\")\n",
    "    if s.upper().startswith(\"GPS\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_approximate(loc):\n",
    "    if pd.isna(loc): return False\n",
    "    loc = str(loc).strip()\n",
    "    # Match \"NE of Burkina Faso\", \"southern part of Ghana\", etc.\n",
    "    direction = r\"(?:N|S|E|W|NE|NW|SE|SW|North|South|East|West|Northern|Southern|Eastern|Western)\"\n",
    "    if re.match(rf\"^{direction}(\\s+part)?\\s+of\\s+.+\", loc, flags=re.IGNORECASE):\n",
    "        return True\n",
    "    if re.match(rf\"^.+\\b({direction})\\b\", loc, flags=re.IGNORECASE):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_geounit(s):\n",
    "    geo_unit_keywords = [\n",
    "        \"belt\", \"craton\", \"basin\", \"shear zone\", \"fault\", \"batholith\", \"terrane\", \"inlier\",\n",
    "        \"province\", \"region\", \"mine\", \"group\", \"supergroup\", \"pluton\", \"complex\", \"gneiss\", \"channel\", \"graben\",\"domain\"\n",
    "    ]\n",
    "    s = s.lower().strip()\n",
    "    # Don't count plain country names as geounit\n",
    "    return any(kw in s for kw in geo_unit_keywords)\n",
    "\n",
    "def classify_location_type(loc):\n",
    "    loc = str(loc)\n",
    "    if is_gps(loc):\n",
    "        return \"GPS\"\n",
    "    elif is_approximate(loc):\n",
    "        return \"APPROXIMATE\"\n",
    "    elif is_geounit(loc):\n",
    "        return \"GEOLOGICAL_UNIT\"\n",
    "    elif len(loc) < 3:\n",
    "        return \"UNGEOCODED\"\n",
    "    else:\n",
    "        return \"PLACE\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhx1bg98pYo4"
   },
   "source": [
    "While geocoding the approximate locations, the base area is extracted and treated as the main location but the geocode will indicate it will be approximate location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "tFUnsvJVe40p"
   },
   "outputs": [],
   "source": [
    "def extract_base_place(location):\n",
    "    \"\"\"\n",
    "    Attempt to extract the most likely base location from a regional phrase.\n",
    "    - Strips out directional modifiers and keeps the likely toponyms.\n",
    "    - Works best for phrases like \"eastern part of X\", \"near Y\", etc.\n",
    "    \"\"\"\n",
    "    # 1. Try 'of ...' or 'in ...'\n",
    "    match = re.search(r'of ([A-Za-z \\-\\’\\'éèêàôîïç]+)$', location)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    match = re.search(r'in ([A-Za-z \\-\\’\\'éèêàôîïç]+)$', location)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    # 2. Remove phrases like 'part of', 'region of', 'border with'\n",
    "    cleaned = re.sub(r'^(north|south|east|west|northern|southern|eastern|western|central|upper|lower|border|area|part|region|near|around|about|along)\\s+of\\s+', '', location, flags=re.I)\n",
    "    cleaned = re.sub(r'^(north|south|east|west|northern|southern|eastern|western|central|upper|lower|border|area|part|region|near|around|about|along)\\s+', '', cleaned, flags=re.I)\n",
    "    cleaned = cleaned.strip(\",.;:()[] \")\n",
    "\n",
    "    # 3. If there are coordinates, skip\n",
    "    if re.search(r'\\d{4,}', cleaned):\n",
    "        return location.strip()\n",
    "\n",
    "    # 4. Return the last capitalized chunk (if any)\n",
    "    matches = re.findall(r'\\b([A-Z][a-zA-Z\\'\\-éèêàôîïç]+(?: [A-Z][a-zA-Z\\'\\-éèêàôîïç]+)*)\\b', cleaned)\n",
    "    if matches:\n",
    "        return matches[-1]\n",
    "\n",
    "    return cleaned.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6e3zF129pqLa"
   },
   "source": [
    "Geocoding using GeoPy geocoders\n",
    "\n",
    "Output will add on: Latitude, Longitude, Location address and geocode type(approximate)\n",
    "\n",
    "If a location is not found then no-geocode result will be output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "dzDBQSRgbv5w"
   },
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent=\"geo_coding_example\")\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)  # Respect OSM's rate limits!\n",
    "\n",
    "# Geocode function with handling for failures\n",
    "def geopy_geocode(loc):\n",
    "    if not loc or loc.lower() in {\"gps\", \"na\", \"no-geocode-result\"}:\n",
    "        return pd.Series([\"no-geocode-result\"] * 4)\n",
    "    # Try full location first\n",
    "    try:\n",
    "        result = geocode(loc, language='en', addressdetails=True, timeout=10)\n",
    "        if result:\n",
    "            return pd.Series([result.latitude, result.longitude, result.address, \"APPROXIMATE\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Error geocoding '{loc}': {e}\")\n",
    "        # You can sleep here for rate limits if needed\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Try base place extraction as fallback\n",
    "    base = extract_base_place(loc)\n",
    "    if base and base.lower() != loc.lower():\n",
    "        try:\n",
    "            result = geocode(base, language='en', addressdetails=True, timeout=10)\n",
    "            if result:\n",
    "                return pd.Series([result.latitude, result.longitude, result.address, \"APPROXIMATE\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Error geocoding fallback '{base}': {e}\")\n",
    "            time.sleep(1)\n",
    "\n",
    "    return pd.Series([\"no-geocode-result\"] * 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "5v0ktE0nliRC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def is_valid_location(loc):\n",
    "    \"\"\"Reject citations, years, and empty strings.\"\"\"\n",
    "    if not isinstance(loc, str) or not loc.strip():\n",
    "        return False\n",
    "    if \"et al\" in loc.lower():\n",
    "        return False\n",
    "    if re.search(r\"\\b(19|20)\\d{2}\\b\", loc):  # any year like 1998, 2006\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def filter_out_authors(df):\n",
    "    \"\"\"Filter rows that look like author names instead of real places.\"\"\"\n",
    "    denylist = {\"potrel\",\"björklund\",\"eriksson\",\"milési\",\"villeneuve\",\n",
    "                \"attoh\",\"ekwueme\",\"shanmugan\",\"sultan\"}\n",
    "    mask = df['location'].apply(\n",
    "        lambda x: is_valid_location(x) and x.lower() not in denylist\n",
    "    )\n",
    "    return df[mask].reset_index(drop=True)\n",
    "\n",
    "def clean_location_csv_folder(input_folder, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    for fname in os.listdir(input_folder):\n",
    "        if not fname.lower().endswith(\".csv\"):\n",
    "            continue\n",
    "        print(f\"Processing: {fname}\")\n",
    "        df = pd.read_csv(os.path.join(input_folder, fname))\n",
    "        original_len = len(df)\n",
    "\n",
    "        # Clean location column\n",
    "        if 'location' in df.columns:\n",
    "            df['location'] = df['location'].map(remove_generic_location_words)\n",
    "        else:\n",
    "            print(f\"Skipping {fname}: no 'location' column\")\n",
    "            continue\n",
    "\n",
    "        # Drop empty after cleaning\n",
    "        df_cleaned = df[df['location'].astype(str).str.strip().astype(bool)].reset_index(drop=True)\n",
    "\n",
    "        # Remove authors (denylist + regex)\n",
    "        if {'location', 'mentions'}.issubset(df_cleaned.columns):\n",
    "            df_cleaned = filter_out_authors(df_cleaned)\n",
    "\n",
    "        # Classify location type\n",
    "        if 'location' in df_cleaned.columns:\n",
    "            df_cleaned['location_type'] = df_cleaned['location'].map(classify_location_type)\n",
    "        else:\n",
    "            df_cleaned['location_type'] = 'UNGEOCODED'\n",
    "\n",
    "        print(f\"Kept {len(df_cleaned)}/{original_len} locations after filtering.\")\n",
    "        # Save\n",
    "        outpath = os.path.join(output_folder, fname.replace(\".csv\", \"_final.csv\"))\n",
    "        df_cleaned.to_csv(outpath, index=False)\n",
    "        print(f\"Saved cleaned: {outpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BdOX5MVpll_3",
    "outputId": "f18ea749-818d-451e-f033-66e4ddd54e5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 2007_tshibubudze_themarkoyefault_2007_merged.csv\n",
      "Kept 104/105 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2007_tshibubudze_themarkoyefault_2007_merged_final.csv\n",
      "Processing: 2008_matabane_fe3_merged.csv\n",
      "Kept 146/146 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2008_matabane_fe3_merged_final.csv\n",
      "Processing: 2009_bontlenkuna_0605886p_honoursreport_merged.csv\n",
      "Kept 86/89 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2009_bontlenkuna_0605886p_honoursreport_merged_final.csv\n",
      "Processing: 2010_matsheka_irvinfinalthesis_merged.csv\n",
      "Kept 104/104 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2010_matsheka_irvinfinalthesis_merged_final.csv\n",
      "Processing: 2010_mohale_gisinterpretationofneburkinafaso_merged.csv\n",
      "Kept 93/94 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2010_mohale_gisinterpretationofneburkinafaso_merged_final.csv\n",
      "Processing: 2011_peters_eastmarkoye_2011_merged.csv\n",
      "Kept 154/155 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2011_peters_eastmarkoye_2011_merged_final.csv\n",
      "Processing: 2011_woolfe_thestratigraphyandmetamorphicfaciesofthekemb_merged.csv\n",
      "Kept 73/73 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2011_woolfe_thestratigraphyandmetamorphicfaciesofthekemb_merged_final.csv\n",
      "Processing: 2012_simoko_petrology,geochemistryandstructureofthepissilabatholithandthesaabazonegneiss_merged.csv\n",
      "Kept 53/54 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2012_simoko_petrology,geochemistryandstructureofthepissilabatholithandthesaabazonegneiss_merged_final.csv\n",
      "Processing: 2013_funyufunyu_merged.csv\n",
      "Kept 324/324 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2013_funyufunyu_merged_final.csv\n",
      "Processing: 2013_peters_merged.csv\n",
      "Kept 295/295 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2013_peters_merged_final.csv\n",
      "Processing: 2013_ramabulana_sadiolahillpetrology_merged.csv\n",
      "Kept 86/88 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2013_ramabulana_sadiolahillpetrology_merged_final.csv\n",
      "Processing: 2014_msc_yossi_merged.csv\n",
      "Kept 281/282 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2014_msc_yossi_merged_final.csv\n",
      "Processing: 2015_lebrun_siguiri_merged.csv\n",
      "Kept 526/528 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2015_lebrun_siguiri_merged_final.csv\n",
      "Processing: 2015_masurel_phd_merged.csv\n",
      "Kept 657/659 locations after filtering.\n",
      "Saved cleaned: DATA_FOR_MODELS/Data_merged_clean\\2015_masurel_phd_merged_final.csv\n"
     ]
    }
   ],
   "source": [
    "clean_location_csv_folder(\"DATA_FOR_MODELS/Data4_merged2\", \"DATA_FOR_MODELS/Data_merged_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4h4cN66WycbX",
    "outputId": "7ec1d6b8-0f93-4a6a-f6d9-13ec7c81853a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geocoding: 2007_tshibubudze_themarkoyefault_2007_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2007_tshibubudze_themarkoyefault_2007_merged_final_geocoded.csv\n",
      "Geocoding: 2008_matabane_fe3_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2008_matabane_fe3_merged_final_geocoded.csv\n",
      "Geocoding: 2009_bontlenkuna_0605886p_honoursreport_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2009_bontlenkuna_0605886p_honoursreport_merged_final_geocoded.csv\n",
      "Geocoding: 2010_matsheka_irvinfinalthesis_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2010_matsheka_irvinfinalthesis_merged_final_geocoded.csv\n",
      "Geocoding: 2010_mohale_gisinterpretationofneburkinafaso_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2010_mohale_gisinterpretationofneburkinafaso_merged_final_geocoded.csv\n",
      "Geocoding: 2011_peters_eastmarkoye_2011_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2011_peters_eastmarkoye_2011_merged_final_geocoded.csv\n",
      "Geocoding: 2011_woolfe_thestratigraphyandmetamorphicfaciesofthekemb_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2011_woolfe_thestratigraphyandmetamorphicfaciesofthekemb_merged_final_geocoded.csv\n",
      "Geocoding: 2012_simoko_petrology,geochemistryandstructureofthepissilabatholithandthesaabazonegneiss_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2012_simoko_petrology,geochemistryandstructureofthepissilabatholithandthesaabazonegneiss_merged_final_geocoded.csv\n",
      "Geocoding: 2013_funyufunyu_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2013_funyufunyu_merged_final_geocoded.csv\n",
      "Geocoding: 2013_peters_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2013_peters_merged_final_geocoded.csv\n",
      "Geocoding: 2013_ramabulana_sadiolahillpetrology_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2013_ramabulana_sadiolahillpetrology_merged_final_geocoded.csv\n",
      "Geocoding: 2014_msc_yossi_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2014_msc_yossi_merged_final_geocoded.csv\n",
      "Geocoding: 2015_lebrun_siguiri_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2015_lebrun_siguiri_merged_final_geocoded.csv\n",
      "Geocoding: 2015_masurel_phd_merged_final.csv\n",
      "Saved geocoded file: DATA_FOR_MODELS/Data_geocoded\\2015_masurel_phd_merged_final_geocoded.csv\n"
     ]
    }
   ],
   "source": [
    "def geocode_location_csv_folder(input_folder, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    for fname in os.listdir(input_folder):\n",
    "        if not fname.lower().endswith(\".csv\"):\n",
    "            continue\n",
    "        print(f\"Geocoding: {fname}\")\n",
    "        df = pd.read_csv(os.path.join(input_folder, fname))\n",
    "        if 'location' not in df.columns:\n",
    "            print(f\"SKIP (no location column): {fname}\")\n",
    "            continue\n",
    "        # Clean generic location words (optional)\n",
    "        df['location'] = df['location'].map(remove_generic_location_words)\n",
    "        # Apply geopy_geocode, safe handling!\n",
    "        geo_df = df['location'].apply(lambda loc: pd.Series(geopy_geocode(loc)))\n",
    "        geo_df.columns = ['geocode_lat', 'geocode_lon', 'geocode_str', 'geocode_type']\n",
    "        df = pd.concat([df, geo_df], axis=1)\n",
    "        out_csv = os.path.join(output_folder, fname.replace(\".csv\", \"_geocoded.csv\"))\n",
    "        df.to_csv(out_csv, index=False)\n",
    "        print(f\"Saved geocoded file: {out_csv}\")\n",
    "\n",
    "\n",
    "geocode_location_csv_folder(\"DATA_FOR_MODELS/Data_merged_clean\", \"DATA_FOR_MODELS/Data_geocoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2007_tshibubudze_themarkoyefault_2007_merged_final_geocoded_map.html\n",
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2008_matabane_fe3_merged_final_geocoded_map.html\n",
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2009_bontlenkuna_0605886p_honoursreport_merged_final_geocoded_map.html\n",
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2010_matsheka_irvinfinalthesis_merged_final_geocoded_map.html\n",
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2010_mohale_gisinterpretationofneburkinafaso_merged_final_geocoded_map.html\n",
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2011_peters_eastmarkoye_2011_merged_final_geocoded_map.html\n",
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2011_woolfe_thestratigraphyandmetamorphicfaciesofthekemb_merged_final_geocoded_map.html\n",
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2012_simoko_petrology,geochemistryandstructureofthepissilabatholithandthesaabazonegneiss_merged_final_geocoded_map.html\n",
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2013_funyufunyu_merged_final_geocoded_map.html\n",
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2013_peters_merged_final_geocoded_map.html\n",
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2013_ramabulana_sadiolahillpetrology_merged_final_geocoded_map.html\n",
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2014_msc_yossi_merged_final_geocoded_map.html\n",
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2015_lebrun_siguiri_merged_final_geocoded_map.html\n",
      "Saved map: DATA_FOR_MODELS\\Data_Maps\\2015_masurel_phd_merged_final_geocoded_map.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Paths ---\n",
    "input_dir = Path(\"DATA_FOR_MODELS/Data_geocoded\")\n",
    "output_dir = Path(\"DATA_FOR_MODELS/Data_Maps\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Function to build a map for one file ---\n",
    "def build_map(csv_path, output_dir):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # keep only valid coords\n",
    "    df = df[df['geocode_lat'].apply(lambda x: str(x).replace('.', '', 1).isdigit())]\n",
    "    df['geocode_lat'] = df['geocode_lat'].astype(float)\n",
    "    df['geocode_lon'] = df['geocode_lon'].astype(float)\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\" Skipping {csv_path.name} (no valid coordinates)\")\n",
    "        return\n",
    "\n",
    "    # --- accuracy: exact or approximate ---\n",
    "    def accuracy(row):\n",
    "        gstr = str(row.get(\"geocode_str\", \"\")).lower()\n",
    "        if \"approx\" in gstr or \"near\" in gstr:\n",
    "            return \"approximate\"\n",
    "        return \"exact\"\n",
    "\n",
    "    df[\"accuracy\"] = df.apply(accuracy, axis=1)\n",
    "\n",
    "    # --- assign colors automatically per unique location_type ---\n",
    "    unique_types = df[\"location_type\"].dropna().unique()\n",
    "    color_cycle = itertools.cycle(\n",
    "        [\"blue\",\"green\",\"red\",\"purple\",\"orange\",\"cyan\",\"pink\",\"brown\",\"gray\",\"olive\"]\n",
    "    )\n",
    "    COLOR_MAP = {lt: next(color_cycle) for lt in unique_types}\n",
    "\n",
    "    def color_for(row):\n",
    "        return COLOR_MAP.get(row.get(\"location_type\", \"\"), \"black\")\n",
    "\n",
    "    # --- center map ---\n",
    "    center_lat = df[\"geocode_lat\"].mean()\n",
    "    center_lon = df[\"geocode_lon\"].mean()\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=6)\n",
    "\n",
    "    # --- plot circles ---\n",
    "    for _, row in df.iterrows():\n",
    "        popup_text = (\n",
    "            f\"<b>Location:</b> {row.get('location_clean', '')}<br>\"\n",
    "            f\"<b>Context:</b> {row.get('mentions', row.get('mention', ''))}<br>\"\n",
    "            f\"<b>Geocode:</b> {row.get('geocode_str', '')}<br>\"\n",
    "            f\"<b>Pages:</b> {row.get('pages', '')}<br>\"\n",
    "            f\"<b>Type:</b> {row.get('location_type', '')}<br>\"\n",
    "            f\"<b>Accuracy:</b> {row['accuracy']}\"\n",
    "        )\n",
    "\n",
    "        folium.CircleMarker(\n",
    "            location=[row[\"geocode_lat\"], row[\"geocode_lon\"]],\n",
    "            radius=7,\n",
    "            color=color_for(row),\n",
    "            fill=True,\n",
    "            fill_color=color_for(row),\n",
    "            fill_opacity=0.7 if row[\"accuracy\"] == \"exact\" else 0.3,\n",
    "            tooltip=row.get(\"location_clean\", \"\"),\n",
    "            popup=folium.Popup(popup_text, max_width=350),\n",
    "        ).add_to(m)\n",
    "\n",
    "    # --- build legend dynamically ---\n",
    "    legend_html = \"\"\"\n",
    "    <div style=\"position: fixed; bottom: 20px; left: 20px; z-index: 9999;\n",
    "                background: white; padding: 8px 10px; border: 1px solid #ccc; border-radius: 4px;\">\n",
    "      <div style=\"font-weight: 600; margin-bottom: 6px;\">Legend (location_type)</div>\n",
    "    \"\"\"\n",
    "    for lt, col in COLOR_MAP.items():\n",
    "        legend_html += f\"\"\"\n",
    "          <div><span style=\"display:inline-block;width:12px;height:12px;background:{col};\n",
    "                             margin-right:6px;border-radius:50%;\"></span>{lt}</div>\n",
    "        \"\"\"\n",
    "    legend_html += \"\"\"\n",
    "      <div style=\"margin-top:6px;\"><i>Transparency: solid = exact, faint = approximate</i></div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    m.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "    # --- save map ---\n",
    "    out_path = output_dir / (csv_path.stem + \"_map.html\")\n",
    "    m.save(out_path)\n",
    "    print(f\"Saved map: {out_path}\")\n",
    "\n",
    "# --- Process all CSV files ---\n",
    "for csv_file in input_dir.glob(\"*.csv\"):\n",
    "    build_map(csv_file, output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
