{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zUnPbStX6Bnf",
    "outputId": "440e5816-3cdd-4d89-b0f3-5f8170b16f66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer.six\n",
      "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six) (3.4.3)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six) (43.0.3)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.23)\n",
      "Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pdfminer.six\n",
      "Successfully installed pdfminer.six-20250506\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bumPOqAp6LH7"
   },
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHNCicGC6S5C"
   },
   "source": [
    "Extract pages and page numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "35iG-Ahw6M6B"
   },
   "outputs": [],
   "source": [
    "def extract_pages_text(pdf_path):\n",
    "    \"\"\" Input: PDF files\n",
    "        Output: list of pages as text(strings)\n",
    "    \"\"\"\n",
    "    pages = []\n",
    "    for page_layout in extract_pages(pdf_path):\n",
    "        lines = []\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                lines.append(element.get_text())\n",
    "        page_text = '\\n'.join(lines)\n",
    "        pages.append(page_text)\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fkJTJrb6dYI"
   },
   "source": [
    "Filter out noise: Cover-page,resumes,declaration, acknowledgements, table of contents,empty lines and References that may have names and locations that will hinder the location extraction. Left with main body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DcrnbUI16XzE"
   },
   "outputs": [],
   "source": [
    "def is_toc_page(text):\n",
    "    if \"table of contents\" in text or \"contents\" in text:\n",
    "        return True\n",
    "    if re.search(r'\\.{5,}', text) and re.search(r'\\d{1,3}\\s*$', text, re.MULTILINE):\n",
    "        return True\n",
    "    if sum(1 for l in text.split('\\n') if re.match(r'.*\\d{1,3}\\s*$', l)) > 5:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_ack_page(text):\n",
    "    return \"acknowledgement\" in text or \"acknowledgments\" in text\n",
    "\n",
    "def is_declaration_page(text):\n",
    "    return \"declaration\" in text\n",
    "\n",
    "def is_main_section_start(text):\n",
    "    return bool(re.search(\n",
    "        r'\\b(?:1\\.|chapter\\s*1)[:\\s-]*introduction\\b|\\bintroduction\\b',\n",
    "        text, re.IGNORECASE\n",
    "    ))\n",
    "\n",
    "def remove_empty_lines(text):\n",
    "    return \"\\n\".join(line for line in text.splitlines() if line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TfZcGYYX7LDJ"
   },
   "outputs": [],
   "source": [
    "def remove_references_sections(page_texts):\n",
    "    \"\"\"\n",
    "    Removes 'References' sections from each page and drops any blank pages.\n",
    "    Input: list of page_texts (strings)\n",
    "    Output: list of cleaned page_texts (strings)\n",
    "    \"\"\"\n",
    "    cleaned_pages = []\n",
    "    skip_mode = False\n",
    "    for page in page_texts:\n",
    "        lines = page.splitlines()\n",
    "        cleaned_lines = []\n",
    "        for line in lines:\n",
    "            # Detect references header\n",
    "            if not skip_mode and re.match(r'^\\s*(\\d+\\.?)?\\s*references\\b', line, re.I):\n",
    "                skip_mode = True\n",
    "                continue\n",
    "            # Exit skip mode if a new section/chapter starts\n",
    "            if skip_mode and (\n",
    "                re.match(r'^\\s*(chapter|paper|section|abstract|introduction)\\b', line, re.I) or\n",
    "                re.match(r'^\\s*(\\d+\\.?)?\\s*(abstract|introduction|chapter|paper|section)\\b', line, re.I)\n",
    "            ):\n",
    "                skip_mode = False\n",
    "            if not skip_mode:\n",
    "                cleaned_lines.append(line)\n",
    "        # Remove empty lines\n",
    "        non_empty = [l for l in cleaned_lines if l.strip()]\n",
    "        # If after cleaning, page is not blank, keep it\n",
    "        if non_empty:\n",
    "            cleaned_pages.append('\\n'.join(non_empty))\n",
    "    return cleaned_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlgXU0VU7moR"
   },
   "source": [
    "PDF to text with page mapping that will help with lookup later. Notably only the main body is converted to text. Coverpage is set to page 1 like what is viewed when pdf is read on a reading application e.g. Adobe Acrobat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Is0DXUGf7j-p"
   },
   "outputs": [],
   "source": [
    "def pdf_to_text_with_page_mapping(pdf_path):\n",
    "    pages = extract_pages_text(pdf_path)\n",
    "    body_pages = pages[1:]  # Removes cover page\n",
    "    filtered_pages = []\n",
    "    kept_pages = []\n",
    "    skip_mode = None\n",
    "    original_page_numbers = list(range(2, len(pages)+1))\n",
    "\n",
    "    for idx, pg in enumerate(body_pages):\n",
    "        pg_lower = pg.lower()\n",
    "        if skip_mode == 'toc':\n",
    "            if is_main_section_start(pg_lower):\n",
    "                skip_mode = None\n",
    "            elif is_toc_page(pg_lower):\n",
    "                continue\n",
    "            else:\n",
    "                skip_mode = None\n",
    "        elif skip_mode == 'ack':\n",
    "            if is_main_section_start(pg_lower):\n",
    "                skip_mode = None\n",
    "            elif is_ack_page(pg_lower):\n",
    "                continue\n",
    "            else:\n",
    "                skip_mode = None\n",
    "        elif skip_mode == 'dec':\n",
    "            if is_main_section_start(pg_lower):\n",
    "                skip_mode = None\n",
    "            elif is_declaration_page(pg_lower):\n",
    "                continue\n",
    "            else:\n",
    "                skip_mode = None\n",
    "        if skip_mode is None:\n",
    "            if is_toc_page(pg_lower):\n",
    "                skip_mode = 'toc'\n",
    "                continue\n",
    "            elif is_ack_page(pg_lower):\n",
    "                skip_mode = 'ack'\n",
    "                continue\n",
    "            elif is_declaration_page(pg_lower):\n",
    "                skip_mode = 'dec'\n",
    "                continue\n",
    "        filtered_pages.append(pg)\n",
    "        kept_pages.append(original_page_numbers[idx])\n",
    "\n",
    "    main_body_text = \"\\n\\n\".join(remove_empty_lines(pg) for pg in filtered_pages)\n",
    "\n",
    "    return main_body_text, kept_pages, filtered_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bo6BHQiT8GQA"
   },
   "source": [
    "Next to be done on multiple PDF academic theses. The output is a folder of text files to be used as input in location extraction. A page count report can be generated to check that the extraction has actually extracted main body and not removed relevant parts that are important especially since these are unstructured pdfs that may have editing that may cause issues with extraction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "kWDAlGGH8Upy"
   },
   "outputs": [],
   "source": [
    "def process_pdf_folder(folder_path, output_txt_folder=None, csv_report_path=None):\n",
    "    \"\"\"\n",
    "    Processes all PDFs in the given folder:\n",
    "    - Extracts text (and keeps track of page numbers).\n",
    "    - Removes all content from the References section onwards.\n",
    "    - Writes cleaned text to .txt files (with page markers).\n",
    "    - Prints and optionally saves a count per file as CSV.\n",
    "    \"\"\"\n",
    "    if output_txt_folder:\n",
    "        os.makedirs(output_txt_folder, exist_ok=True)\n",
    "    report = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            print(f\"Processing: {filename}\")\n",
    "            try:\n",
    "                main_text, kept_page_nums, kept_page_texts = pdf_to_text_with_page_mapping(pdf_path)\n",
    "                kept_page_texts_norefs = remove_references_sections(kept_page_texts)\n",
    "                kept_page_nums_norefs = kept_page_nums[:len(kept_page_texts_norefs)]\n",
    "\n",
    "                if output_txt_folder:\n",
    "                    txt_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "                    txt_path = os.path.join(output_txt_folder, txt_filename)\n",
    "                    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                        for page_num, page_text in zip(kept_page_nums_norefs, kept_page_texts_norefs):\n",
    "                            f.write(f\"\\n--- Page {page_num} ---\\n\")\n",
    "                            f.write(remove_empty_lines(page_text).strip() + \"\\n\")\n",
    "                report.append({\"filename\": filename, \"kept_pages\": len(kept_page_nums_norefs)})\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {filename}: {e}\")\n",
    "    print(\"\\n=== Page Count Report ===\")\n",
    "    for row in report:\n",
    "        print(f\"{row['filename']}: {row['kept_pages']} pages kept\")\n",
    "    if csv_report_path:\n",
    "        pd.DataFrame(report).to_csv(csv_report_path, index=False)\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "snJqo_r58aaJ",
    "outputId": "88571dd5-1a12-4032-cf0b-840dc78091b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 2007_Tshibubudze_THE MARKOYE FAULT_2007.pdf\n",
      "Processing: 2008_MATABANE_FE3.pdf\n",
      "Processing: 2009_Bontle Nkuna_0605886P_Honours Report.pdf\n",
      "Processing: 2010_Matsheka_Irvin Final Thesis.pdf\n",
      "Processing: 2010_Mohale_GIS interpretation of NE Burkina Faso.pdf\n",
      "Processing: 2011_Peters_East Markoye_2011.pdf\n",
      "Processing: 2011_Woolfe_The stratigraphy and metamorphic facies of the KEMB.pdf\n",
      "Processing: 2012_Simoko_Petrology, geochemistry and structure of the Pissila batholith and the Saaba Zone gneiss.pdf\n",
      "Processing: 2013_FUNYUFUNYU.pdf\n",
      "Processing: 2013_Peters.pdf\n",
      "Processing: 2013_Ramabulana_Sadiola Hill petrology.pdf\n",
      "Processing: 2014_MSc_YOSSI.pdf\n",
      "Processing: 2015_LeBrun_Siguiri.pdf\n",
      "Processing: 2015_Masurel_phd.pdf\n",
      "\n",
      "=== Page Count Report ===\n",
      "2007_Tshibubudze_THE MARKOYE FAULT_2007.pdf: 49 pages kept\n",
      "2008_MATABANE_FE3.pdf: 39 pages kept\n",
      "2009_Bontle Nkuna_0605886P_Honours Report.pdf: 45 pages kept\n",
      "2010_Matsheka_Irvin Final Thesis.pdf: 28 pages kept\n",
      "2010_Mohale_GIS interpretation of NE Burkina Faso.pdf: 41 pages kept\n",
      "2011_Peters_East Markoye_2011.pdf: 50 pages kept\n",
      "2011_Woolfe_The stratigraphy and metamorphic facies of the KEMB.pdf: 50 pages kept\n",
      "2012_Simoko_Petrology, geochemistry and structure of the Pissila batholith and the Saaba Zone gneiss.pdf: 32 pages kept\n",
      "2013_FUNYUFUNYU.pdf: 71 pages kept\n",
      "2013_Peters.pdf: 77 pages kept\n",
      "2013_Ramabulana_Sadiola Hill petrology.pdf: 34 pages kept\n",
      "2014_MSc_YOSSI.pdf: 38 pages kept\n",
      "2015_LeBrun_Siguiri.pdf: 191 pages kept\n",
      "2015_Masurel_phd.pdf: 227 pages kept\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    folder = \"DATA_FOR_MODELS/data_pdf\"\n",
    "    out_folder = \"DATA_FOR_MODELS/Data_txt\"\n",
    "    report_csv = \"DATA_FOR_MODELS/pdf_page_report.csv\"\n",
    "    process_pdf_folder(folder, out_folder, report_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
